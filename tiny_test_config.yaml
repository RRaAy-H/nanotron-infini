# Tiny test configuration for Infini-Llama
# For validating preprocessing and training workflow with minimal data

model:
  name: llama
  hidden_size: 512
  n_heads: 8
  n_layers: 4
  mlp_ratio: 2.0
  vocab_size: 32000
  max_position_embeddings: 2048
  rope_theta: 10000.0
  tie_word_embeddings: false

tokenizer:
  tokenizer_name_or_path: "meta-llama/Llama-2-7b-hf"

optimizer:
  name: adamw
  lr: 3.0e-4
  betas: [0.9, 0.95]
  eps: 1.0e-8
  weight_decay: 0.1

scheduler:
  name: cosine_with_warmup
  warmup_steps: 10
  max_lr: 3.0e-4
  min_lr: 3.0e-5

tokens:
  train_steps: 50  # Very few steps for testing
  checkpoint_every: 25
  val_every: 25
  val_steps: 5
  global_batch_size: 8
  sequence_length: 1024

data_stages:
  - name: dummy
    start_training_step: 0
    data:
      num_loading_workers: 1
      seed: 42
      dataset:
        hf_dataset_or_datasets: "/data1/dataset/HuggingFaceFW/processed/tiny"  # Path to tiny dataset
        hf_dataset_splits: ["train"]
        hf_dataset_config_name: "default"
        text_column_name: "text"
        dataset_processing_num_proc_per_process: 1
        dataset_overwrite_cache: true

checkpointing:
  checkpoint_dir: "./tiny_test_checkpoints"
  checkpoint_every: 25
  resume_checkpoint_path: null
  checkpoint_latest_path: null

parallelism:
  dp_size: 1
  pp_size: 1
  tp_size: 1
  pp_engine: 1f1b

logging:
  log_level: info
  log_level_replica: warning
  log_every: 5

random_seed: 42

# Infini-Attention settings
infini_attention:
  segment_length: 32
  turn_on_memory: true
  balance_init_type: 0
  balance_act_type: 0
  balance_factor_lr: 0.0
