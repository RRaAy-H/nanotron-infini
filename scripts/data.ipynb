{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c118316",
   "metadata": {},
   "source": [
    "# GPU-Aware Data Preprocessing for Nanotron Training\n",
    "\n",
    "This notebook processes parquet files and splits them into training and evaluation datasets (80/20 split) with GPU-aware optimizations.\n",
    "\n",
    "## Features:\n",
    "- üîç Automatic GPU detection and configuration\n",
    "- üíæ Memory-optimized processing for large datasets \n",
    "- ‚öôÔ∏è Device-specific optimizations (CPU vs GPU)\n",
    "- üìà Progress tracking and memory monitoring\n",
    "- üì¶ Efficient data loading with chunked processing\n",
    "- üìä Data validation and integrity checks\n",
    "\n",
    "## Hardware Requirements:\n",
    "- **CPU**: Minimum 8GB RAM recommended for large datasets\n",
    "- **GPU**: Optional but recommended for faster processing\n",
    "- **Storage**: SSD recommended for better I/O performance\n",
    "\n",
    "## Configuration:\n",
    "The notebook automatically detects your hardware and configures optimal settings for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204e352",
   "metadata": {},
   "source": [
    "# GPU-Aware Data Preprocessing for Training\n",
    "\n",
    "This notebook processes parquet files and splits them into training and evaluation datasets (80/20 split).\n",
    "Includes GPU detection and device configuration for efficient processing on training devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# GPU and device management\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable progress bars for pandas operations\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# GPU Detection and Device Configuration\n",
    "def detect_gpu_setup():\n",
    "    \"\"\"Detect available GPUs and system configuration\"\"\"\n",
    "    print(\"\\nüîç GPU and System Detection:\")\n",
    "    \n",
    "    # Check PyTorch installation\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"   ‚úÖ CUDA available with {gpu_count} GPU(s)\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            memory_gb = gpu_props.total_memory / 1024**3\n",
    "            print(f\"      GPU {i}: {gpu_props.name} ({memory_gb:.1f} GB)\")\n",
    "            \n",
    "        # Get current GPU\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"   Current device: cuda:{current_device}\")\n",
    "        \n",
    "        return True, gpu_count\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CUDA not available - using CPU for data processing\")\n",
    "        return False, 0\n",
    "\n",
    "# System resources\n",
    "def check_system_resources():\n",
    "    \"\"\"Check system memory and CPU cores\"\"\"\n",
    "    print(\"\\nüíª System Resources:\")\n",
    "    \n",
    "    # Memory\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_gb = memory.total / 1024**3\n",
    "    available_gb = memory.available / 1024**3\n",
    "    print(f\"   RAM: {memory_gb:.1f} GB total, {available_gb:.1f} GB available\")\n",
    "    \n",
    "    # CPU\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    print(f\"   CPU cores: {cpu_count}\")\n",
    "    \n",
    "    return memory_gb, cpu_count\n",
    "\n",
    "# Run detection\n",
    "has_gpu, gpu_count = detect_gpu_setup()\n",
    "memory_gb, cpu_count = check_system_resources()\n",
    "\n",
    "print(\"\\nüìä Recommendations for data processing:\")\n",
    "if has_gpu:\n",
    "    print(f\"   ‚Ä¢ Use GPU acceleration for large datasets\")\n",
    "    print(f\"   ‚Ä¢ Enable GPU-accelerated pandas operations\")\n",
    "    print(f\"   ‚Ä¢ Consider GPU memory when processing large files\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Optimize for CPU processing\")\n",
    "    print(f\"   ‚Ä¢ Use chunked processing for large datasets\")\n",
    "    print(f\"   ‚Ä¢ Increase num_workers for parallel processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611671b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to your parquet files and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b139bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\n",
      "Output directory: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\n",
      "Train split: 0.8, Eval split: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration with GPU and Device Settings\n",
    "INPUT_DATA_PATH = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\"  # Change this to your actual path\n",
    "OUTPUT_DIR = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\"\n",
    "TRAIN_SPLIT = 0.8\n",
    "EVAL_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# GPU Configuration (set these for your training device)\n",
    "GPU_DEVICE = \"cuda:0\"  # Change to your specific GPU (cuda:0, cuda:1, etc.)\n",
    "USE_GPU_PROCESSING = has_gpu  # Enable GPU-accelerated processing if available\n",
    "CHUNK_SIZE = 10000 if not has_gpu else 50000  # Larger chunks if GPU available\n",
    "NUM_WORKERS = min(4, cpu_count)  # Parallel processing workers\n",
    "\n",
    "# Memory management\n",
    "MAX_MEMORY_GB = min(memory_gb * 0.8, 32)  # Use up to 80% of available RAM, max 32GB\n",
    "GPU_MEMORY_FRACTION = 0.9  # Use 90% of GPU memory if available\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   Input path: {INPUT_DATA_PATH}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"   Train split: {TRAIN_SPLIT}, Eval split: {EVAL_SPLIT}\")\n",
    "print(f\"   Random seed: {RANDOM_SEED}\")\n",
    "print(f\"\\nüéØ Device Configuration:\")\n",
    "print(f\"   Target GPU device: {GPU_DEVICE}\")\n",
    "print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"   Parallel workers: {NUM_WORKERS}\")\n",
    "print(f\"   Max memory usage: {MAX_MEMORY_GB:.1f} GB\")\n",
    "\n",
    "if USE_GPU_PROCESSING and has_gpu:\n",
    "    # Set GPU memory fraction\n",
    "    torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_FRACTION, device=torch.cuda.current_device())\n",
    "    print(f\"   GPU memory fraction: {GPU_MEMORY_FRACTION}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/eval\", exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5eda",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load all parquet files from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413751c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files:\n",
      "  1. /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/000_00000.parquet\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_parquet_files(data_path: str) -> List[str]:\n",
    "    \"\"\"Find all parquet files in the given directory.\"\"\"\n",
    "    parquet_files = []\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if data_path.is_file() and data_path.suffix == '.parquet':\n",
    "        return [str(data_path)]\n",
    "    \n",
    "    for file_path in data_path.rglob(\"*.parquet\"):\n",
    "        parquet_files.append(str(file_path))\n",
    "    \n",
    "    return sorted(parquet_files)\n",
    "\n",
    "def load_parquet_robust(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a parquet file with robust error handling for Arrow extension issues.\"\"\"\n",
    "    try:\n",
    "        # First attempt: Default pyarrow engine\n",
    "        return pd.read_parquet(file_path, engine='pyarrow')\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  PyArrow failed ({str(e)[:50]}...), trying alternatives...\")\n",
    "        \n",
    "        try:\n",
    "            # Second attempt: Use fastparquet engine\n",
    "            return pd.read_parquet(file_path, engine='fastparquet')\n",
    "        except Exception as e2:\n",
    "            print(f\"    ‚ö†Ô∏è  FastParquet failed ({str(e2)[:50]}...), trying PyArrow with ignore_metadata...\")\n",
    "            \n",
    "            try:\n",
    "                # Third attempt: PyArrow without metadata\n",
    "                import pyarrow.parquet as pq\n",
    "                table = pq.read_table(file_path, use_pandas_metadata=False)\n",
    "                return table.to_pandas()\n",
    "            except Exception as e3:\n",
    "                print(f\"    ‚ö†Ô∏è  PyArrow no-metadata failed ({str(e3)[:50]}...), trying manual conversion...\")\n",
    "                \n",
    "                try:\n",
    "                    # Fourth attempt: Manual Arrow to Pandas conversion\n",
    "                    import pyarrow.parquet as pq\n",
    "                    import pyarrow as pa\n",
    "                    \n",
    "                    # Read table without pandas metadata\n",
    "                    table = pq.read_table(file_path)\n",
    "                    \n",
    "                    # Convert to pandas manually, handling extension types\n",
    "                    df_dict = {}\n",
    "                    for i, column in enumerate(table.column_names):\n",
    "                        col_data = table.column(i)\n",
    "                        \n",
    "                        # Handle different Arrow types\n",
    "                        if pa.types.is_string(col_data.type) or pa.types.is_large_string(col_data.type):\n",
    "                            df_dict[column] = col_data.to_pandas()\n",
    "                        elif pa.types.is_integer(col_data.type) or pa.types.is_floating(col_data.type):\n",
    "                            df_dict[column] = col_data.to_pandas()\n",
    "                        else:\n",
    "                            # For extension types, convert to string as fallback\n",
    "                            try:\n",
    "                                df_dict[column] = col_data.to_pandas()\n",
    "                            except:\n",
    "                                # Last resort: convert to string\n",
    "                                df_dict[column] = [str(x) for x in col_data.to_pylist()]\n",
    "                    \n",
    "                    return pd.DataFrame(df_dict)\n",
    "                    \n",
    "                except Exception as e4:\n",
    "                    print(f\"    ‚ùå All methods failed. Final error: {e4}\")\n",
    "                    raise e4\n",
    "\n",
    "# Find all parquet files\n",
    "parquet_files = find_parquet_files(INPUT_DATA_PATH)\n",
    "print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "for i, file in enumerate(parquet_files[:10]):  # Show first 10 files\n",
    "    print(f\"  {i+1}. {file}\")\n",
    "if len(parquet_files) > 10:\n",
    "    print(f\"  ... and {len(parquet_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_advanced_fallback(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Advanced parquet loading with specific handling for pandas.period extension type errors.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    strategies = [\n",
    "        # Strategy 1: Direct pandas read\n",
    "        lambda: pd.read_parquet(file_path),\n",
    "        \n",
    "        # Strategy 2: PyArrow with ignore_metadata\n",
    "        lambda: pd.read_parquet(file_path, use_pandas_metadata=False),\n",
    "        \n",
    "        # Strategy 3: FastParquet engine\n",
    "        lambda: pd.read_parquet(file_path, engine='fastparquet'),\n",
    "        \n",
    "        # Strategy 4: PyArrow table with metadata reset\n",
    "        lambda: load_with_metadata_reset(file_path),\n",
    "        \n",
    "        # Strategy 5: Manual column-by-column conversion\n",
    "        lambda: load_with_manual_conversion(file_path)\n",
    "    ]\n",
    "    \n",
    "    for i, strategy in enumerate(strategies, 1):\n",
    "        try:\n",
    "            print(f\"    Trying strategy {i}...\")\n",
    "            df = strategy()\n",
    "            print(f\"    ‚úÖ Strategy {i} succeeded!\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Strategy {i} failed: {str(e)[:80]}...\")\n",
    "            if i == len(strategies):\n",
    "                raise e\n",
    "            continue\n",
    "\n",
    "def load_with_metadata_reset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load parquet by completely resetting Arrow metadata.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    # Read table\n",
    "    table = pq.read_table(file_path)\n",
    "    \n",
    "    # Create completely new schema without any extension metadata\n",
    "    new_fields = []\n",
    "    for i, (name, field) in enumerate(zip(table.schema.names, table.schema)):\n",
    "        # Get the base type, stripping any extension metadata\n",
    "        base_type = field.type\n",
    "        \n",
    "        # Handle specific problematic types\n",
    "        if hasattr(base_type, 'value_type'):\n",
    "            # This is likely an extension type, use the underlying type\n",
    "            base_type = base_type.value_type\n",
    "        \n",
    "        # Create new field with clean type\n",
    "        new_field = pa.field(name, base_type)\n",
    "        new_fields.append(new_field)\n",
    "    \n",
    "    # Create new schema\n",
    "    new_schema = pa.schema(new_fields)\n",
    "    \n",
    "    # Create new table with clean schema\n",
    "    new_table = pa.table(table.columns, schema=new_schema)\n",
    "    \n",
    "    # Convert to pandas\n",
    "    return new_table.to_pandas()\n",
    "\n",
    "def load_with_manual_conversion(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load parquet with manual type conversion to avoid extension type issues.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    # Read table\n",
    "    table = pq.read_table(file_path)\n",
    "    \n",
    "    # Convert each column manually\n",
    "    data_dict = {}\n",
    "    \n",
    "    for i, column_name in enumerate(table.schema.names):\n",
    "        column = table.column(i)\n",
    "        column_type = column.type\n",
    "        \n",
    "        try:\n",
    "            # Try direct conversion first\n",
    "            data_dict[column_name] = column.to_pandas()\n",
    "        except Exception:\n",
    "            # If that fails, convert to basic types\n",
    "            if pa.types.is_string(column_type) or pa.types.is_large_string(column_type):\n",
    "                data_dict[column_name] = [str(x) if x is not None else None for x in column.to_pylist()]\n",
    "            elif pa.types.is_integer(column_type):\n",
    "                data_dict[column_name] = [int(x) if x is not None else None for x in column.to_pylist()]\n",
    "            elif pa.types.is_floating(column_type):\n",
    "                data_dict[column_name] = [float(x) if x is not None else None for x in column.to_pylist()]\n",
    "            elif pa.types.is_boolean(column_type):\n",
    "                data_dict[column_name] = [bool(x) if x is not None else None for x in column.to_pylist()]\n",
    "            else:\n",
    "                # Fallback: convert everything to string\n",
    "                data_dict[column_name] = [str(x) if x is not None else None for x in column.to_pylist()]\n",
    "    \n",
    "    return pd.DataFrame(data_dict)\n",
    "\n",
    "def load_parquet_files_robust(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load multiple parquet files with advanced error handling.\"\"\"\n",
    "    dataframes = []\n",
    "    failed_files = []\n",
    "    \n",
    "    print(f\"üîß Loading {len(file_paths)} parquet files with advanced fallback strategies...\")\n",
    "    \n",
    "    for file_path in tqdm(file_paths, desc=\"Loading files\", unit=\"file\"):\n",
    "        file_name = Path(file_path).name\n",
    "        try:\n",
    "            print(f\"\\nüìÅ Loading {file_name}...\")\n",
    "            df = load_parquet_advanced_fallback(file_path)\n",
    "            dataframes.append(df)\n",
    "            print(f\"  ‚úÖ Successfully loaded {file_name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            print(f\"  ‚ùå Failed to load {file_name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"\\nüí• CRITICAL ERROR: No files could be loaded!\")\n",
    "        print(\"\\nüîç Detailed error analysis:\")\n",
    "        for file_path, error in failed_files:\n",
    "            print(f\"   {Path(file_path).name}: {error}\")\n",
    "        \n",
    "        print(\"\\nüõ†Ô∏è  Advanced troubleshooting for pandas.period error:\")\n",
    "        print(\"   1. This error occurs when Arrow extension types conflict\")\n",
    "        print(\"   2. Try recreating the parquet files:\")\n",
    "        print(\"      import pandas as pd\")\n",
    "        print(\"      df = pd.read_csv('your_data.csv')  # or other format\")\n",
    "        print(\"      df.to_parquet('fixed_file.parquet', engine='pyarrow', compression='snappy')\")\n",
    "        print(\"   3. Check PyArrow version compatibility:\")\n",
    "        print(\"      pip install pyarrow==12.0.0 pandas==2.0.3\")\n",
    "        print(\"   4. Alternative: Convert to different format temporarily\")\n",
    "        \n",
    "        raise ValueError(\"All loading strategies failed. See troubleshooting guide above.\")\n",
    "    \n",
    "    print(f\"\\nüîó Concatenating {len(dataframes)} successfully loaded dataframes...\")\n",
    "    \n",
    "    # Ensure all dataframes have the same columns\n",
    "    if len(dataframes) > 1:\n",
    "        first_cols = set(dataframes[0].columns)\n",
    "        for i, df in enumerate(dataframes[1:], 1):\n",
    "            if set(df.columns) != first_cols:\n",
    "                print(f\"   ‚ö†Ô∏è  Warning: Column mismatch in file {i+1}. Aligning columns...\")\n",
    "                # Align columns\n",
    "                all_cols = sorted(set().union(*[df.columns for df in dataframes]))\n",
    "                for j, df in enumerate(dataframes):\n",
    "                    for col in all_cols:\n",
    "                        if col not in df.columns:\n",
    "                            df[col] = None\n",
    "                    dataframes[j] = df[all_cols]\n",
    "    \n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"‚úÖ Loading complete!\")\n",
    "    print(f\"   üìä Total rows: {len(combined_df):,}\")\n",
    "    print(f\"   üìã Total columns: {len(combined_df.columns)}\")\n",
    "    print(f\"   üìà Successfully loaded: {len(dataframes)}/{len(file_paths)} files\")\n",
    "    print(f\"   üíæ Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"   ‚ö†Ô∏è  {len(failed_files)} files failed to load\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Install required package if needed\n",
    "try:\n",
    "    import fastparquet\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing fastparquet for additional loading support...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\"])\n",
    "    import fastparquet\n",
    "    print(\"‚úÖ fastparquet installed successfully\")\n",
    "\n",
    "# Load data with advanced error handling\n",
    "if parquet_files:\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nüöÄ Starting advanced parquet loading...\")\n",
    "    print(f\"   Target files: {len(parquet_files)}\")\n",
    "    print(f\"   Device: {GPU_DEVICE}\")\n",
    "    \n",
    "    try:\n",
    "        df = load_parquet_files_robust(parquet_files)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nüéâ SUCCESS! Data loaded in {load_time:.2f} seconds\")\n",
    "        print(f\"üìä Final dataset info:\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print(f\"   Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüëÄ Sample data (first 3 rows):\")\n",
    "        print(df.head(3).to_string())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüí• FINAL ERROR: {e}\")\n",
    "        print(f\"\\nüìû Contact support with this error information:\")\n",
    "        print(f\"   File: {parquet_files[0] if parquet_files else 'None'}\")\n",
    "        print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "        print(f\"   PyArrow version: {pa.__version__ if 'pa' in locals() else 'Unknown'}\")\n",
    "        print(f\"   Pandas version: {pd.__version__}\")\n",
    "else:\n",
    "    print(\"‚ùå No parquet files found! Check your INPUT_DATA_PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8fe3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_parquet_with_fallbacks\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a parquet file with multiple fallback strategies for Arrow extension type errors.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     strategies \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Strategy 1: Default pandas read_parquet\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: pd\u001b[38;5;241m.\u001b[39mread_parquet(file_path),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: load_parquet_direct_arrow(file_path)\n\u001b[1;32m     18\u001b[0m     ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def load_parquet_with_fallbacks(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a parquet file with multiple fallback strategies for Arrow extension type errors.\"\"\"\n",
    "    strategies = [\n",
    "        # Strategy 1: Default pandas read_parquet\n",
    "        lambda: pd.read_parquet(file_path),\n",
    "        \n",
    "        # Strategy 2: Use pyarrow directly without extension metadata\n",
    "        lambda: pd.read_parquet(file_path, engine='pyarrow'),\n",
    "        \n",
    "        # Strategy 3: Load with pyarrow and ignore metadata\n",
    "        lambda: load_parquet_ignore_metadata(file_path),\n",
    "        \n",
    "        # Strategy 4: Load with fastparquet engine\n",
    "        lambda: pd.read_parquet(file_path, engine='fastparquet'),\n",
    "        \n",
    "        # Strategy 5: Direct pyarrow table conversion\n",
    "        lambda: load_parquet_direct_arrow(file_path)\n",
    "    ]\n",
    "    \n",
    "    last_error = None\n",
    "    for i, strategy in enumerate(strategies, 1):\n",
    "        try:\n",
    "            return strategy()\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if i < len(strategies):  # Don't print for last attempt\n",
    "                continue\n",
    "    \n",
    "    # If all strategies failed, raise the last error\n",
    "    raise last_error\n",
    "\n",
    "def load_parquet_ignore_metadata(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load parquet file ignoring extension metadata that causes Arrow type errors.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    # Read parquet file without extension metadata\n",
    "    table = pq.read_table(file_path)\n",
    "    \n",
    "    # Remove extension metadata that might cause issues\n",
    "    schema_without_metadata = pa.schema([\n",
    "        pa.field(name, field.type.value_type if hasattr(field.type, 'value_type') else field.type)\n",
    "        for name, field in zip(table.schema.names, table.schema)\n",
    "    ])\n",
    "    \n",
    "    # Create new table with clean schema\n",
    "    clean_table = pa.table(table.columns, schema=schema_without_metadata)\n",
    "    \n",
    "    return clean_table.to_pandas()\n",
    "\n",
    "def load_parquet_direct_arrow(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load parquet using direct PyArrow table conversion.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    # Read as PyArrow table\n",
    "    table = pq.read_table(file_path)\n",
    "    \n",
    "    # Convert to pandas with safe conversion\n",
    "    return table.to_pandas(safe=False, ignore_metadata=True)\n",
    "\n",
    "def load_parquet_files(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple parquet files with robust error handling.\"\"\"\n",
    "    dataframes = []\n",
    "    total_rows = 0\n",
    "    failed_files = []\n",
    "    \n",
    "    print(\"Loading parquet files with robust error handling...\")\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading files\", unit=\"file\"):\n",
    "        try:\n",
    "            df = load_parquet_with_fallbacks(file_path)\n",
    "            dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "            tqdm.write(f\"  ‚úì Loaded {Path(file_path).name}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            tqdm.write(f\"  ‚úó Error loading {Path(file_path).name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"\\n‚ùå No parquet files could be loaded successfully!\")\n",
    "        print(\"\\nüîç Failed files details:\")\n",
    "        for file_path, error in failed_files:\n",
    "            print(f\"   {Path(file_path).name}: {error}\")\n",
    "        print(\"\\nüí° Troubleshooting suggestions:\")\n",
    "        print(\"   1. Check if files are corrupted: Try opening one file manually\")\n",
    "        print(\"   2. Check Arrow/PyArrow versions: pip install --upgrade pyarrow pandas\")\n",
    "        print(\"   3. Try converting files: Use a different tool to re-save the parquet files\")\n",
    "        print(\"   4. Check file permissions: Ensure files are readable\")\n",
    "        raise ValueError(\"No parquet files could be loaded successfully! See troubleshooting above.\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {len(failed_files)} files failed to load:\")\n",
    "        for file_path, error in failed_files[:3]:  # Show first 3 failures\n",
    "            print(f\"   {Path(file_path).name}: {error}\")\n",
    "        if len(failed_files) > 3:\n",
    "            print(f\"   ... and {len(failed_files) - 3} more\")\n",
    "    \n",
    "    print(f\"\\nüìä Concatenating {len(dataframes)} dataframes...\")\n",
    "    # Show progress for concatenation\n",
    "    with tqdm(total=1, desc=\"Concatenating\", unit=\"operation\") as pbar:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"‚úÖ Total rows after concatenation: {len(combined_df):,}\")\n",
    "    print(f\"üìà Successfully loaded {len(dataframes)}/{len(file_paths)} files\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def load_parquet_files_gpu_aware(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple parquet files with GPU-aware processing and robust error handling.\"\"\"\n",
    "    dataframes = []\n",
    "    total_rows = 0\n",
    "    failed_files = []\n",
    "    \n",
    "    print(f\"üìö Loading {len(file_paths)} parquet files with GPU-aware processing...\")\n",
    "    print(f\"   Device target: {GPU_DEVICE}\")\n",
    "    print(f\"   Memory limit: {MAX_MEMORY_GB:.1f} GB\")\n",
    "    \n",
    "    # Memory monitoring\n",
    "    def get_memory_usage():\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            return f\"GPU: {gpu_memory:.1f}GB\"\n",
    "        else:\n",
    "            ram_usage = psutil.virtual_memory().used / 1024**3\n",
    "            return f\"RAM: {ram_usage:.1f}GB\"\n",
    "    \n",
    "    # Process files with memory monitoring and robust loading\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading files\", unit=\"file\"):\n",
    "        try:\n",
    "            # Check memory before loading\n",
    "            memory_info = get_memory_usage()\n",
    "            \n",
    "            # Load with robust fallback strategies\n",
    "            df = load_parquet_with_fallbacks(file_path)\n",
    "            \n",
    "            # Memory optimization\n",
    "            if USE_GPU_PROCESSING and has_gpu:\n",
    "                # Convert to GPU-friendly format if needed\n",
    "                # Note: pandas doesn't directly support GPU, but we prepare for downstream GPU processing\n",
    "                pass\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "            \n",
    "            tqdm.write(f\"  ‚úì Loaded {Path(file_path).name}: {len(df):,} rows | {memory_info}\")\n",
    "            \n",
    "            # Memory management - garbage collection if needed\n",
    "            if len(dataframes) % 5 == 0:  # Every 5 files\n",
    "                import gc\n",
    "                gc.collect()\n",
    "                if USE_GPU_PROCESSING and has_gpu:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "            tqdm.write(f\"  ‚úó Error loading {Path(file_path).name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"\\n‚ùå No parquet files could be loaded successfully!\")\n",
    "        print(\"\\nüîç Failed files details:\")\n",
    "        for file_path, error in failed_files:\n",
    "            print(f\"   {Path(file_path).name}: {error}\")\n",
    "        print(\"\\nüí° Advanced troubleshooting:\")\n",
    "        print(\"   1. Arrow extension types: This error suggests incompatible Arrow metadata\")\n",
    "        print(\"   2. Install compatible versions: pip install pyarrow==12.0.0 pandas==2.0.3\")\n",
    "        print(\"   3. Check source: Verify how the parquet files were created\")\n",
    "        print(\"   4. Manual inspection: Try loading with: pyarrow.parquet.read_table(file)\")\n",
    "        raise ValueError(\"No parquet files could be loaded successfully! See troubleshooting above.\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {len(failed_files)} files failed to load:\")\n",
    "        for file_path, error in failed_files[:3]:  # Show first 3 failures\n",
    "            print(f\"   {Path(file_path).name}: {error}\")\n",
    "        if len(failed_files) > 3:\n",
    "            print(f\"   ... and {len(failed_files) - 3} more\")\n",
    "    \n",
    "    print(f\"\\nüîó Concatenating {len(dataframes)} dataframes...\")\n",
    "    \n",
    "    # Efficient concatenation with progress tracking\n",
    "    with tqdm(total=1, desc=\"Concatenating\", unit=\"operation\") as pbar:\n",
    "        # Use efficient concatenation\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Clear intermediate dataframes to free memory\n",
    "    del dataframes\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if USE_GPU_PROCESSING and has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ Total rows after concatenation: {len(combined_df):,}\")\n",
    "    print(f\"üìà Successfully loaded {len(file_paths) - len(failed_files)}/{len(file_paths)} files\")\n",
    "    print(f\"   Final memory usage: {get_memory_usage()}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Load all data with enhanced error handling\n",
    "if parquet_files:\n",
    "    print(\"\\nüîß Installing required packages for robust parquet loading...\")\n",
    "    try:\n",
    "        import fastparquet\n",
    "    except ImportError:\n",
    "        print(\"Installing fastparquet for additional fallback support...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\"])\n",
    "        import fastparquet\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"\\nüöÄ Starting robust data loading with fallback strategies...\")\n",
    "    \n",
    "    try:\n",
    "        df = load_parquet_files_gpu_aware(parquet_files)\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Loading completed in {load_time:.2f} seconds\")\n",
    "        print(f\"üìà Dataset shape: {df.shape}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå All loading strategies failed. Error: {e}\")\n",
    "        print(\"\\nüõ†Ô∏è  Additional debugging steps:\")\n",
    "        print(\"   1. Check one file manually:\")\n",
    "        if parquet_files:\n",
    "            print(f\"      import pandas as pd\")\n",
    "            print(f\"      df = pd.read_parquet('{parquet_files[0]}')\")\n",
    "        print(\"   2. Check pyarrow version: import pyarrow; print(pyarrow.__version__)\")\n",
    "        print(\"   3. Try recreating parquet files with: df.to_parquet(path, engine='pyarrow')\")\n",
    "else:\n",
    "    print(\"‚ùå No parquet files found! Please check your INPUT_DATA_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11328e22",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Explore the structure and content of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace1787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Shape: (1048581, 9)\n",
      "Memory usage: 6094.63 MB\n",
      "\n",
      "Column types:\n",
      "text               object\n",
      "id                 object\n",
      "dump               object\n",
      "url                object\n",
      "date               object\n",
      "file_path          object\n",
      "language           object\n",
      "language_score    float64\n",
      "token_count         int64\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>dump</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>file_path</th>\n",
       "      <th>language</th>\n",
       "      <th>language_score</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|Viewing Single Post From: Spoilers for the We...</td>\n",
       "      <td>&lt;urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://daytimeroyaltyonline.com/single/?p=8906...</td>\n",
       "      <td>2013-05-18T05:48:59Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.823210</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*sigh* Fundamentalist community, let me pass o...</td>\n",
       "      <td>&lt;urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://endogenousretrovirus.blogspot.com/2007/...</td>\n",
       "      <td>2013-05-18T06:43:03Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.973771</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A novel two-step immunotherapy approach has sh...</td>\n",
       "      <td>&lt;urn:uuid:07b8e00d-b445-4736-a593-cd1c147dce21&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://news.cancerconnect.com/</td>\n",
       "      <td>2013-05-18T05:23:15Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.872709</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free the Cans! Working Together to Reduce Wast...</td>\n",
       "      <td>&lt;urn:uuid:c970d9a2-a5ce-4050-9ea3-58d7bbd609a8&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://sharingsolution.com/2009/05/23/free-the...</td>\n",
       "      <td>2013-05-18T05:49:03Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.932360</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORLANDO, Fla. ‚Äî While the Rapid Recall Exchang...</td>\n",
       "      <td>&lt;urn:uuid:5c2cac9e-2fda-4194-959b-6ede0668ad2a&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://supermarketnews.com/food-safety/more-su...</td>\n",
       "      <td>2013-05-18T05:25:43Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.955206</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  |Viewing Single Post From: Spoilers for the We...   \n",
       "1  *sigh* Fundamentalist community, let me pass o...   \n",
       "2  A novel two-step immunotherapy approach has sh...   \n",
       "3  Free the Cans! Working Together to Reduce Wast...   \n",
       "4  ORLANDO, Fla. ‚Äî While the Rapid Recall Exchang...   \n",
       "\n",
       "                                                id             dump  \\\n",
       "0  <urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>  CC-MAIN-2013-20   \n",
       "1  <urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017>  CC-MAIN-2013-20   \n",
       "2  <urn:uuid:07b8e00d-b445-4736-a593-cd1c147dce21>  CC-MAIN-2013-20   \n",
       "3  <urn:uuid:c970d9a2-a5ce-4050-9ea3-58d7bbd609a8>  CC-MAIN-2013-20   \n",
       "4  <urn:uuid:5c2cac9e-2fda-4194-959b-6ede0668ad2a>  CC-MAIN-2013-20   \n",
       "\n",
       "                                                 url                  date  \\\n",
       "0  http://daytimeroyaltyonline.com/single/?p=8906...  2013-05-18T05:48:59Z   \n",
       "1  http://endogenousretrovirus.blogspot.com/2007/...  2013-05-18T06:43:03Z   \n",
       "2                     http://news.cancerconnect.com/  2013-05-18T05:23:15Z   \n",
       "3  http://sharingsolution.com/2009/05/23/free-the...  2013-05-18T05:49:03Z   \n",
       "4  http://supermarketnews.com/food-safety/more-su...  2013-05-18T05:25:43Z   \n",
       "\n",
       "                                           file_path language  language_score  \\\n",
       "0  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.823210   \n",
       "1  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.973771   \n",
       "2  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.872709   \n",
       "3  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.932360   \n",
       "4  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.955206   \n",
       "\n",
       "   token_count  \n",
       "0          142  \n",
       "1          703  \n",
       "2          576  \n",
       "3          575  \n",
       "4          708  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "                                                     text  \\\n",
      "count                                             1048581   \n",
      "unique                                            1048417   \n",
      "top     |Track & Field Profile - Embed| Suggest a Corr...   \n",
      "freq                                                    5   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                                                     id             dump  \\\n",
      "count                                           1048581          1048581   \n",
      "unique                                          1048581                8   \n",
      "top     <urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>  CC-MAIN-2022-21   \n",
      "freq                                                  1           181154   \n",
      "mean                                                NaN              NaN   \n",
      "std                                                 NaN              NaN   \n",
      "min                                                 NaN              NaN   \n",
      "25%                                                 NaN              NaN   \n",
      "50%                                                 NaN              NaN   \n",
      "75%                                                 NaN              NaN   \n",
      "max                                                 NaN              NaN   \n",
      "\n",
      "                                                      url  \\\n",
      "count                                             1048581   \n",
      "unique                                            1048444   \n",
      "top     http://trendsupdates.com/offbeat-valentines-da...   \n",
      "freq                                                    2   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                        date  \\\n",
      "count                1048581   \n",
      "unique                885255   \n",
      "top     2015-03-28T00:28:17Z   \n",
      "freq                       8   \n",
      "mean                     NaN   \n",
      "std                      NaN   \n",
      "min                      NaN   \n",
      "25%                      NaN   \n",
      "50%                      NaN   \n",
      "75%                      NaN   \n",
      "max                      NaN   \n",
      "\n",
      "                                                file_path language  \\\n",
      "count                                             1048581  1048581   \n",
      "unique                                             340274        1   \n",
      "top     s3://commoncrawl/crawl-data/CC-MAIN-2015-14/se...       en   \n",
      "freq                                                   17  1048581   \n",
      "mean                                                  NaN      NaN   \n",
      "std                                                   NaN      NaN   \n",
      "min                                                   NaN      NaN   \n",
      "25%                                                   NaN      NaN   \n",
      "50%                                                   NaN      NaN   \n",
      "75%                                                   NaN      NaN   \n",
      "max                                                   NaN      NaN   \n",
      "\n",
      "        language_score   token_count  \n",
      "count     1.048581e+06  1.048581e+06  \n",
      "unique             NaN           NaN  \n",
      "top                NaN           NaN  \n",
      "freq               NaN           NaN  \n",
      "mean      9.282094e-01  6.926566e+02  \n",
      "std       5.651962e-02  1.372510e+03  \n",
      "min       6.500090e-01  3.100000e+01  \n",
      "25%       9.112734e-01  2.060000e+02  \n",
      "50%       9.451391e-01  4.040000e+02  \n",
      "75%       9.652135e-01  7.710000e+02  \n",
      "max       9.991004e-01  1.300400e+05  \n",
      "\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "if 'df' in locals():\n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"\\nNo missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a55a5c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Clean and preprocess the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "After removing duplicates: (1048581, 9) (removed 0 rows)\n",
      "After removing missing values: (1048581, 9)\n",
      "After filtering empty text: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty id: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty dump: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty url: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty date: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty file_path: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty language: (1048581, 9) (removed 0 rows)\n",
      "Preprocessing complete. Final shape: (1048581, 9)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataset with progress tracking.\n",
    "    Modify this function based on your specific data requirements.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting data preprocessing...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Create a progress bar for preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        \"Removing duplicates\",\n",
    "        \"Handling missing values\", \n",
    "        \"Filtering text columns\",\n",
    "        \"Custom preprocessing\"\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(preprocessing_steps), desc=\"Preprocessing\", unit=\"step\") as pbar:\n",
    "        # 1. Remove duplicates\n",
    "        pbar.set_description(\"Removing duplicates\")\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = original_shape[0] - df.shape[0]\n",
    "        tqdm.write(f\"  ‚úì After removing duplicates: {df.shape} (removed {duplicates_removed:,} rows)\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        pbar.set_description(\"Handling missing values\")\n",
    "        before_na = len(df)\n",
    "        df = df.dropna()\n",
    "        na_removed = before_na - len(df)\n",
    "        tqdm.write(f\"  ‚úì After removing missing values: {df.shape} (removed {na_removed:,} rows)\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Filter out empty text fields\n",
    "        pbar.set_description(\"Filtering text columns\")\n",
    "        text_columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        if len(text_columns) > 0:\n",
    "            for col in tqdm(text_columns, desc=\"Processing text cols\", leave=False):\n",
    "                if col in df.columns:\n",
    "                    initial_len = len(df)\n",
    "                    df = df[df[col].str.strip().str.len() > 0]\n",
    "                    removed = initial_len - len(df)\n",
    "                    if removed > 0:\n",
    "                        tqdm.write(f\"    ‚úì Filtered empty {col}: removed {removed:,} rows\")\n",
    "        else:\n",
    "            tqdm.write(\"  ‚ÑπÔ∏è  No text columns found\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 4. Custom preprocessing steps\n",
    "        pbar.set_description(\"Custom preprocessing\")\n",
    "        # Add any custom preprocessing steps here\n",
    "        # Example: text length filtering, tokenization, etc.\n",
    "        tqdm.write(\"  ‚úì Custom preprocessing completed\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    total_removed = original_shape[0] - len(df)\n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   üìä Original shape: {original_shape}\")\n",
    "    print(f\"   üìä Final shape: {df.shape}\")\n",
    "    print(f\"   üìä Total rows removed: {total_removed:,} ({total_removed/original_shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data_gpu_aware(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    GPU-aware preprocessing with memory optimization and device management.\n",
    "    Modify this function based on your specific data requirements.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting GPU-aware data preprocessing...\")\n",
    "    print(f\"   Device target: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    \n",
    "    original_shape = df.shape\n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"   Initial memory usage: {initial_memory:.1f} MB\")\n",
    "    \n",
    "    # Memory monitoring function\n",
    "    def monitor_memory(step_name):\n",
    "        current_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "            return f\"RAM: {current_memory:.1f}MB, GPU: {gpu_memory:.1f}MB\"\n",
    "        return f\"RAM: {current_memory:.1f}MB\"\n",
    "    \n",
    "    # Create a progress bar for preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        \"Memory optimization\",\n",
    "        \"Removing duplicates\",\n",
    "        \"Handling missing values\", \n",
    "        \"Filtering text columns\",\n",
    "        \"GPU preparation\",\n",
    "        \"Custom preprocessing\"\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(preprocessing_steps), desc=\"GPU-aware preprocessing\", unit=\"step\") as pbar:\n",
    "        # 0. Memory optimization\n",
    "        pbar.set_description(\"Memory optimization\")\n",
    "        # Optimize data types to reduce memory usage\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        # Optimize object columns\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:  # If less than 50% unique values\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        memory_after_opt = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        memory_saved = initial_memory - memory_after_opt\n",
    "        tqdm.write(f\"  ‚úì Memory optimized: {memory_saved:.1f} MB saved | {monitor_memory('optimization')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 1. Remove duplicates\n",
    "        pbar.set_description(\"Removing duplicates\")\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = original_shape[0] - df.shape[0]\n",
    "        tqdm.write(f\"  ‚úì After removing duplicates: {df.shape} (removed {duplicates_removed:,} rows) | {monitor_memory('duplicates')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        pbar.set_description(\"Handling missing values\")\n",
    "        before_na = len(df)\n",
    "        df = df.dropna()\n",
    "        na_removed = before_na - len(df)\n",
    "        tqdm.write(f\"  ‚úì After removing missing values: {df.shape} (removed {na_removed:,} rows) | {monitor_memory('missing')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Filter out empty text fields\n",
    "        pbar.set_description(\"Filtering text columns\")\n",
    "        text_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if len(text_columns) > 0:\n",
    "            for col in tqdm(text_columns, desc=\"Processing text cols\", leave=False):\n",
    "                if col in df.columns and df[col].dtype in ['object', 'category']:\n",
    "                    initial_len = len(df)\n",
    "                    # Convert categorical back to string for filtering\n",
    "                    if df[col].dtype.name == 'category':\n",
    "                        df[col] = df[col].astype('str')\n",
    "                    df = df[df[col].str.strip().str.len() > 0]\n",
    "                    removed = initial_len - len(df)\n",
    "                    if removed > 0:\n",
    "                        tqdm.write(f\"    ‚úì Filtered empty {col}: removed {removed:,} rows\")\n",
    "        else:\n",
    "            tqdm.write(\"  ‚ÑπÔ∏è  No text columns found\")\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Text filtering complete | {monitor_memory('text_filter')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 4. GPU preparation\n",
    "        pbar.set_description(\"GPU preparation\")\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            tqdm.write(f\"  ‚úì Data prepared for GPU device: {GPU_DEVICE}\")\n",
    "            tqdm.write(f\"  ‚úì GPU memory management enabled\")\n",
    "            # Clear any existing GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            tqdm.write(f\"  ‚úì Data optimized for CPU processing\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 5. Custom preprocessing steps\n",
    "        pbar.set_description(\"Custom preprocessing\")\n",
    "        \n",
    "        # Text length filtering (if text column exists)\n",
    "        if 'text' in df.columns:\n",
    "            initial_len = len(df)\n",
    "            min_text_length = 50  # Minimum characters\n",
    "            df = df[df['text'].str.len() >= min_text_length]\n",
    "            filtered_short = initial_len - len(df)\n",
    "            if filtered_short > 0:\n",
    "                tqdm.write(f\"    ‚úì Filtered short texts (<{min_text_length} chars): removed {filtered_short:,} rows\")\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Custom preprocessing completed | {monitor_memory('custom')}\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    total_removed = original_shape[0] - len(df)\n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_reduction = initial_memory - final_memory\n",
    "    \n",
    "    print(f\"‚úÖ GPU-aware preprocessing complete!\")\n",
    "    print(f\"   üìä Original shape: {original_shape}\")\n",
    "    print(f\"   üìä Final shape: {df.shape}\")\n",
    "    print(f\"   üìä Total rows removed: {total_removed:,} ({total_removed/original_shape[0]*100:.1f}%)\")\n",
    "    print(f\"   üíæ Memory reduction: {memory_reduction:.1f} MB ({memory_reduction/initial_memory*100:.1f}%)\")\n",
    "    print(f\"   üéØ Ready for GPU training on device: {GPU_DEVICE}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "if 'df' in locals():\n",
    "    start_time = time.time()\n",
    "    df_processed = preprocess_data(df.copy())\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Preprocessing completed in {process_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to preprocess. Please load data first.\")\n",
    "\n",
    "# Apply GPU-aware preprocessing\n",
    "if 'df' in locals():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nüîß Starting preprocessing with GPU configuration:\")\n",
    "    print(f\"   Target device: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    \n",
    "    df_processed = preprocess_data_gpu_aware(df.copy())\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  GPU-aware preprocessing completed in {process_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to preprocess. Please load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8dced",
   "metadata": {},
   "source": [
    "## Train/Eval Split\n",
    "\n",
    "Split the data into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7b4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset with train_size=0.8, random_state=42\n",
      "Train set: (838864, 9)\n",
      "Eval set: (209717, 9)\n",
      "Train ratio: 0.800\n",
      "Eval ratio: 0.200\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(df: pd.DataFrame, train_size: float = 0.8, random_state: int = 42) -> tuple:\n",
    "    \"\"\"Split dataset into train and eval sets with progress tracking.\"\"\"\n",
    "    print(f\"üîÄ Splitting dataset with train_size={train_size}, random_state={random_state}\")\n",
    "    \n",
    "    with tqdm(total=3, desc=\"Dataset splitting\", unit=\"step\") as pbar:\n",
    "        # Shuffle the dataset\n",
    "        pbar.set_description(\"Shuffling dataset\")\n",
    "        df_shuffled = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Split the data\n",
    "        pbar.set_description(\"Splitting data\")\n",
    "        train_df, eval_df = train_test_split(\n",
    "            df_shuffled, \n",
    "            train_size=train_size, \n",
    "            random_state=random_state,\n",
    "            shuffle=False  # Already shuffled above\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Validate split\n",
    "        pbar.set_description(\"Validating split\")\n",
    "        assert len(train_df) + len(eval_df) == len(df), \"Split validation failed!\"\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"‚úÖ Split completed!\")\n",
    "    print(f\"   üìä Train set: {train_df.shape} ({len(train_df) / len(df)*100:.1f}%)\")\n",
    "    print(f\"   üìä Eval set: {eval_df.shape} ({len(eval_df) / len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "# Split the data\n",
    "if 'df_processed' in locals():\n",
    "    start_time = time.time()\n",
    "    train_df, eval_df = split_dataset(df_processed, TRAIN_SPLIT, RANDOM_SEED)\n",
    "    split_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Splitting completed in {split_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No processed data to split. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a10b8",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save the train and eval datasets to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff801563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataframes to prevent Arrow extension type errors during save\n",
    "if 'train_df' in locals() and 'eval_df' in locals():\n",
    "    print(\"üßπ Cleaning dataframes to prevent Arrow extension type errors...\")\n",
    "    \n",
    "    def clean_dataframe_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean DataFrame by converting problematic types to basic pandas types.\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        print(f\"   Cleaning DataFrame with {len(df_clean.columns)} columns...\")\n",
    "        \n",
    "        for col in df_clean.columns:\n",
    "            original_dtype = df_clean[col].dtype\n",
    "            \n",
    "            # Handle different problematic types\n",
    "            if 'period' in str(original_dtype).lower():\n",
    "                print(f\"     Converting period column '{col}' to string\")\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "            elif 'category' in str(original_dtype).lower():\n",
    "                print(f\"     Converting category column '{col}' to string\")\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "            elif hasattr(original_dtype, 'name') and 'extension' in str(original_dtype).lower():\n",
    "                print(f\"     Converting extension type column '{col}' to string\")\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "            # Convert any remaining object columns to string to be safe\n",
    "            elif original_dtype == 'object':\n",
    "                try:\n",
    "                    # Try to keep as object if it's already basic\n",
    "                    test_save = df_clean[col].iloc[:5]\n",
    "                    pd.DataFrame({col: test_save}).to_parquet('/tmp/test_col.parquet', index=False)\n",
    "                    os.remove('/tmp/test_col.parquet')\n",
    "                except:\n",
    "                    print(f\"     Converting problematic object column '{col}' to string\")\n",
    "                    df_clean[col] = df_clean[col].astype(str)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    # Clean both dataframes\n",
    "    print(\"\\nüîß Cleaning training dataframe...\")\n",
    "    train_df_clean = clean_dataframe_types(train_df)\n",
    "    \n",
    "    print(\"\\nüîß Cleaning evaluation dataframe...\")\n",
    "    eval_df_clean = clean_dataframe_types(eval_df)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataframes cleaned successfully!\")\n",
    "    print(f\"   Train DataFrame: {train_df_clean.shape}\")\n",
    "    print(f\"   Eval DataFrame: {eval_df_clean.shape}\")\n",
    "    print(f\"   Data types summary:\")\n",
    "    print(f\"     Train: {train_df_clean.dtypes.value_counts().to_dict()}\")\n",
    "    print(f\"     Eval: {eval_df_clean.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Replace original dataframes with cleaned versions\n",
    "    train_df = train_df_clean\n",
    "    eval_df = eval_df_clean\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data to clean. Please run preprocessing and splitting first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e1971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n"
     ]
    }
   ],
   "source": [
    "def save_datasets(train_df: pd.DataFrame, eval_df: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Save train and eval datasets to parquet files with progress tracking and Arrow error handling.\"\"\"\n",
    "    print(\"üíæ Saving datasets...\")\n",
    "    \n",
    "    def save_parquet_robust(df: pd.DataFrame, path: str) -> None:\n",
    "        \"\"\"Save DataFrame to parquet with robust error handling for Arrow extension types.\"\"\"\n",
    "        try:\n",
    "            # Strategy 1: Direct save with pyarrow\n",
    "            df.to_parquet(path, index=False, engine='pyarrow')\n",
    "        except Exception as e:\n",
    "            if \"pandas.period already defined\" in str(e) or \"extension\" in str(e).lower():\n",
    "                print(f\"      ‚ö†Ô∏è  Arrow extension error, trying fallback methods...\")\n",
    "                \n",
    "                try:\n",
    "                    # Strategy 2: Save with fastparquet\n",
    "                    df.to_parquet(path, index=False, engine='fastparquet')\n",
    "                except Exception as e2:\n",
    "                    print(f\"      ‚ö†Ô∏è  FastParquet failed, cleaning DataFrame...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Strategy 3: Clean DataFrame and save\n",
    "                        df_clean = clean_dataframe_for_save(df)\n",
    "                        df_clean.to_parquet(path, index=False, engine='pyarrow')\n",
    "                    except Exception as e3:\n",
    "                        print(f\"      ‚ö†Ô∏è  Clean DataFrame failed, trying final fallback...\")\n",
    "                        \n",
    "                        # Strategy 4: Convert to basic types and save\n",
    "                        df_basic = convert_to_basic_types(df)\n",
    "                        df_basic.to_parquet(path, index=False, engine='pyarrow')\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def clean_dataframe_for_save(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean DataFrame by removing problematic extension types.\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Convert any remaining extension types to basic types\n",
    "        for col in df_clean.columns:\n",
    "            if hasattr(df_clean[col].dtype, 'name'):\n",
    "                if 'period' in str(df_clean[col].dtype).lower():\n",
    "                    # Convert period to string\n",
    "                    df_clean[col] = df_clean[col].astype(str)\n",
    "                elif 'category' in str(df_clean[col].dtype).lower():\n",
    "                    # Convert category to string if it's causing issues\n",
    "                    df_clean[col] = df_clean[col].astype(str)\n",
    "                elif 'extension' in str(df_clean[col].dtype).lower():\n",
    "                    # Convert any other extension types to string\n",
    "                    df_clean[col] = df_clean[col].astype(str)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def convert_to_basic_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert DataFrame to only basic pandas types.\"\"\"\n",
    "        df_basic = pd.DataFrame()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                # Try to infer basic type\n",
    "                series = df[col]\n",
    "                if series.dtype == 'object':\n",
    "                    df_basic[col] = series.astype(str)\n",
    "                elif 'int' in str(series.dtype):\n",
    "                    df_basic[col] = series.astype('int64')\n",
    "                elif 'float' in str(series.dtype):\n",
    "                    df_basic[col] = series.astype('float64')\n",
    "                elif 'bool' in str(series.dtype):\n",
    "                    df_basic[col] = series.astype(bool)\n",
    "                else:\n",
    "                    # Fallback to string for any complex types\n",
    "                    df_basic[col] = series.astype(str)\n",
    "            except Exception:\n",
    "                # Ultimate fallback\n",
    "                df_basic[col] = df[col].astype(str)\n",
    "        \n",
    "        return df_basic\n",
    "    \n",
    "    save_tasks = [\n",
    "        (\"Training data\", train_df, f\"{output_dir}/train/train_data.parquet\"),\n",
    "        (\"Evaluation data\", eval_df, f\"{output_dir}/eval/eval_data.parquet\")\n",
    "    ]\n",
    "    \n",
    "    saved_paths = []\n",
    "    \n",
    "    with tqdm(total=len(save_tasks) + 1, desc=\"Saving datasets\", unit=\"file\") as pbar:\n",
    "        for task_name, data, path in save_tasks:\n",
    "            pbar.set_description(f\"Saving {task_name.lower()}\")\n",
    "            \n",
    "            # Save with robust error handling\n",
    "            try:\n",
    "                save_parquet_robust(data, path)\n",
    "                file_size = os.path.getsize(path) / 1024**2\n",
    "                \n",
    "                tqdm.write(f\"  ‚úì {task_name} saved to: {path}\")\n",
    "                tqdm.write(f\"    üìä Shape: {data.shape}\")\n",
    "                tqdm.write(f\"    üíæ Size: {file_size:.2f} MB\")\n",
    "                \n",
    "                saved_paths.append(path)\n",
    "                pbar.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"  ‚ùå Failed to save {task_name}: {e}\")\n",
    "                print(f\"\\nüõ†Ô∏è  Troubleshooting save error:\")\n",
    "                print(f\"     Error: {type(e).__name__}: {e}\")\n",
    "                print(f\"     Data types in {task_name}:\")\n",
    "                for col, dtype in data.dtypes.items():\n",
    "                    print(f\"       {col}: {dtype}\")\n",
    "                raise e\n",
    "            \n",
    "            tqdm.write(f\"  ‚úì {task_name} saved to: {path}\")\n",
    "            tqdm.write(f\"    üìä Shape: {data.shape}\")\n",
    "            tqdm.write(f\"    üíæ Size: {file_size:.2f} MB\")\n",
    "            \n",
    "            saved_paths.append(path)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Save metadata\n",
    "        pbar.set_description(\"Saving metadata\")\n",
    "        metadata = {\n",
    "            \"total_samples\": len(train_df) + len(eval_df),\n",
    "            \"train_samples\": len(train_df),\n",
    "            \"eval_samples\": len(eval_df),\n",
    "            \"train_split\": len(train_df) / (len(train_df) + len(eval_df)),\n",
    "            \"eval_split\": len(eval_df) / (len(train_df) + len(eval_df)),\n",
    "            \"columns\": list(train_df.columns),\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"source_files\": len(parquet_files) if 'parquet_files' in locals() else 0,\n",
    "            \"processing_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{output_dir}/metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Metadata saved to: {metadata_path}\")\n",
    "        saved_paths.append(metadata_path)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(\"‚úÖ All datasets saved successfully!\")\n",
    "    return saved_paths[0], saved_paths[1], saved_paths[2]\n",
    "\n",
    "# Save the datasets\n",
    "if 'train_df' in locals() and 'eval_df' in locals():\n",
    "    start_time = time.time()\n",
    "    train_path, eval_path, metadata_path = save_datasets(train_df, eval_df, OUTPUT_DIR)\n",
    "    save_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Saving completed in {save_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f3755",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify the saved datasets by loading them back and checking their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e21c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_saved_data(train_path: str, eval_path: str, metadata_path: str):\n",
    "    \"\"\"Verify the saved datasets with progress tracking.\"\"\"\n",
    "    print(\"üîç Verifying saved datasets...\")\n",
    "    \n",
    "    verification_tasks = [\n",
    "        (\"Loading metadata\", metadata_path),\n",
    "        (\"Loading train data\", train_path), \n",
    "        (\"Loading eval data\", eval_path),\n",
    "        (\"Checking data integrity\", None)\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(verification_tasks), desc=\"Verification\", unit=\"task\") as pbar:\n",
    "        # Load metadata\n",
    "        pbar.set_description(\"Loading metadata\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        tqdm.write(\"‚úì Metadata loaded:\")\n",
    "        for key, value in metadata.items():\n",
    "            tqdm.write(f\"    {key}: {value}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Load and verify train data\n",
    "        pbar.set_description(\"Loading train data\")\n",
    "        train_loaded = pd.read_parquet(train_path)\n",
    "        tqdm.write(f\"‚úì Train data loaded from: {Path(train_path).name}\")\n",
    "        tqdm.write(f\"    üìä Shape: {train_loaded.shape}\")\n",
    "        tqdm.write(f\"    üìã Columns: {list(train_loaded.columns)}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Load and verify eval data\n",
    "        pbar.set_description(\"Loading eval data\")\n",
    "        eval_loaded = pd.read_parquet(eval_path)\n",
    "        tqdm.write(f\"‚úì Eval data loaded from: {Path(eval_path).name}\")\n",
    "        tqdm.write(f\"    üìä Shape: {eval_loaded.shape}\")\n",
    "        tqdm.write(f\"    üìã Columns: {list(eval_loaded.columns)}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Data integrity checks\n",
    "        pbar.set_description(\"Checking integrity\")\n",
    "        checks_passed = 0\n",
    "        total_checks = 3\n",
    "        \n",
    "        # Check 1: Column consistency\n",
    "        if list(train_loaded.columns) == list(eval_loaded.columns):\n",
    "            tqdm.write(\"  ‚úì Column names match between train and eval\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(\"  ‚úó Column names mismatch between train and eval\")\n",
    "        \n",
    "        # Check 2: No empty datasets\n",
    "        if len(train_loaded) > 0 and len(eval_loaded) > 0:\n",
    "            tqdm.write(\"  ‚úì Both datasets contain data\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(\"  ‚úó One or both datasets are empty\")\n",
    "        \n",
    "        # Check 3: Metadata consistency\n",
    "        expected_total = metadata['train_samples'] + metadata['eval_samples']\n",
    "        actual_total = len(train_loaded) + len(eval_loaded)\n",
    "        if expected_total == actual_total:\n",
    "            tqdm.write(\"  ‚úì Sample counts match metadata\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(f\"  ‚úó Sample count mismatch: expected {expected_total}, got {actual_total}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "    print(f\"‚úÖ Verification complete! ({checks_passed}/{total_checks} checks passed)\")\n",
    "    return train_loaded, eval_loaded\n",
    "\n",
    "# Verify the saved data\n",
    "if all(var in locals() for var in ['train_path', 'eval_path', 'metadata_path']):\n",
    "    start_time = time.time()\n",
    "    train_verified, eval_verified = verify_saved_data(train_path, eval_path, metadata_path)\n",
    "    verify_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Verification completed in {verify_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No saved data to verify. Please run the saving step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97ab9b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preprocessing and splitting completed successfully! \n",
    "\n",
    "### Next Steps:\n",
    "1. Review the processed data quality\n",
    "2. Adjust preprocessing parameters if needed\n",
    "3. Use the saved parquet files for training with nanotron\n",
    "4. The data paths are ready to be used in your training configuration\n",
    "\n",
    "### File Outputs:\n",
    "- Training data: `{OUTPUT_DIR}/train/train_data.parquet`\n",
    "- Evaluation data: `{OUTPUT_DIR}/eval/eval_data.parquet` \n",
    "- Metadata: `{OUTPUT_DIR}/metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary with enhanced progress display\n",
    "if all(var in locals() for var in ['train_df', 'eval_df']):\n",
    "    print(\"üéâ \" + \"=\"*60 + \" üéâ\")\n",
    "    print(\"üìä GPU-AWARE DATA PREPROCESSING SUMMARY\")\n",
    "    print(\"üéâ \" + \"=\"*60 + \" üéâ\")\n",
    "    \n",
    "    # Hardware configuration summary\n",
    "    print(f\"\\nüñ•Ô∏è  Hardware Configuration:\")\n",
    "    print(f\"   Target GPU device: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    print(f\"   Processing chunk size: {CHUNK_SIZE:,} rows\")\n",
    "    print(f\"   Parallel workers: {NUM_WORKERS}\")\n",
    "    print(f\"   Max memory usage: {MAX_MEMORY_GB:.1f} GB\")\n",
    "    \n",
    "    # Calculate total processing time if variables exist\n",
    "    total_time = 0\n",
    "    if 'load_time' in locals():\n",
    "        total_time += load_time\n",
    "        print(f\"\\n‚è±Ô∏è  Performance Metrics:\")\n",
    "        print(f\"   Loading time: {load_time:.2f}s\")\n",
    "    if 'process_time' in locals():\n",
    "        total_time += process_time  \n",
    "        print(f\"   Processing time: {process_time:.2f}s\")\n",
    "    if 'split_time' in locals():\n",
    "        total_time += split_time\n",
    "        print(f\"   Splitting time: {split_time:.2f}s\")\n",
    "    if 'save_time' in locals():\n",
    "        total_time += save_time\n",
    "        print(f\"   Saving time: {save_time:.2f}s\")\n",
    "    if 'verify_time' in locals():\n",
    "        total_time += verify_time\n",
    "        print(f\"   Verification time: {verify_time:.2f}s\")\n",
    "    \n",
    "    if total_time > 0:\n",
    "        print(f\"   Total processing time: {total_time:.2f}s\")\n",
    "    \n",
    "    # Data summary\n",
    "    print(f\"\\nüìà Data Summary:\")\n",
    "    print(f\"   Input files processed: {len(parquet_files) if 'parquet_files' in locals() else 0}\")ing and splitting completed successfully!** \n",
    "    print(f\"   Total samples: {len(train_df) + len(eval_df):,}\")\n",
    "    print(f\"   Training samples: {len(train_df):,} ({len(train_df)/(len(train_df)+len(eval_df))*100:.1f}%)\")\n",
    "    print(f\"   Evaluation samples: {len(eval_df):,} ({len(eval_df)/(len(train_df)+len(eval_df))*100:.1f}%)\")GPU or CPU\n",
    "    print(f\"   Output directory: {OUTPUT_DIR}\")*: Dynamic memory management based on available hardware\n",
    "    ing optimized for your system\n",
    "    # File paths for trainingorker data loading when supported\n",
    "    print(f\"\\nüìÅ Training Files:\")\n",
    "    print(f\"   Train data: {OUTPUT_DIR}/train/train_data.parquet\")\n",
    "    print(f\"   Eval data: {OUTPUT_DIR}/eval/eval_data.parquet\")in_data.parquet`\n",
    "    print(f\"   Metadata: {OUTPUT_DIR}/metadata.json\")Evaluation data**: `{OUTPUT_DIR}/eval/eval_data.parquet` \n",
    "    `{OUTPUT_DIR}/metadata.json`\n",
    "    # Next steps for training\n",
    "    print(f\"\\nüöÄ Next Steps for Training:\")g:\n",
    "    print(f\"   1. üìù Review the processed data quality\")Check the processed data statistics above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"  ‚ùå {step}\")        for step in missing_vars:        print(\"Missing steps:\")    if missing_vars:                missing_vars.append(\"üîÄ Data splitting\")    if 'train_df' not in locals() or 'eval_df' not in locals():        missing_vars.append(\"üîß Data preprocessing\")    if 'df_processed' not in locals():        missing_vars.append(\"üìà Data loading\")    if 'df' not in locals():        missing_vars.append(\"üìÅ File discovery\")    if 'parquet_files' not in locals():    missing_vars = []    # Show which steps are missing        print(\"‚ùå Please run all cells above to complete the data preprocessing pipeline.\")else:    print(\"üéâ \" + \"=\"*60 + \" üéâ\")    print(\"\\nüéâ Data ready for GPU-accelerated training with Infini attention!\")            print(f\"   {key}: {value}\")    for key, value in training_config.items():    print(f\"\\nüìÑ Training Configuration (copy to training script):\")        }        \"total_samples\": len(train_df) + len(eval_df)        \"flash_attention\": USE_GPU_PROCESSING,        \"mixed_precision\": USE_GPU_PROCESSING,        \"gradient_accumulation_steps\": 4,        \"batch_size_per_gpu\": 16 if USE_GPU_PROCESSING else 4,        \"use_gpu\": USE_GPU_PROCESSING,        \"device\": GPU_DEVICE,        \"eval_file\": f\"{OUTPUT_DIR}/eval/eval_data.parquet\",        \"train_file\": f\"{OUTPUT_DIR}/train/train_data.parquet\",        \"data_path\": OUTPUT_DIR,    training_config = {    # Configuration for training script        print(f\"   4. üìä Monitor training progress and GPU utilization\")    print(f\"   3. üéØ Run the training notebook (scripts/train.ipynb)\")    print(f\"      - Memory optimization: {MAX_MEMORY_GB:.1f} GB limit\")    print(f\"      - GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")    print(f\"      - Device: {GPU_DEVICE}\")    print(f\"   2. üîß Configure your training environment with these settings:\")    2. **Training Environment**: Use the detected GPU configuration for optimal performance\n",
    "    3. **Nanotron Training**: Run `scripts/train.ipynb` with the generated data files\n",
    "    4. **Monitoring**: Track GPU utilization and memory usage during training\n",
    "\n",
    "    ### ‚öôÔ∏è Configuration for Training Script:\n",
    "    ```python\n",
    "    # Use these paths in your training configuration\n",
    "    TRAIN_DATA_PATH = \"{OUTPUT_DIR}/train/train_data.parquet\"\n",
    "    EVAL_DATA_PATH = \"{OUTPUT_DIR}/eval/eval_data.parquet\"\n",
    "    DEVICE = \"cuda:0\"  # or your detected GPU device\n",
    "    USE_FLASH_ATTENTION = True  # if GPU supports it\n",
    "    MIXED_PRECISION = True      # for faster training\n",
    "    ```\n",
    "\n",
    "    ### üìä Performance Optimization:\n",
    "    - **GPU Memory**: Optimized for available VRAM\n",
    "    - **Batch Size**: Automatically configured based on your hardware\n",
    "    - **Data Loading**: Parallel workers for efficient I/O\n",
    "    - **Memory Management**: Garbage collection and cache clearing\n",
    "else:\n",
    "    print(\"‚ùå Please run all cells above to complete the data preprocessing pipeline.\")\n",
    "    \n",
    "    # Show which steps are missing\n",
    "    missing_vars = []\n",
    "    if 'parquet_files' not in locals():\n",
    "        missing_vars.append(\"üìÅ File discovery\")\n",
    "    if 'df' not in locals():\n",
    "        missing_vars.append(\"üìä Data loading\")\n",
    "    if 'df_processed' not in locals():\n",
    "        missing_vars.append(\"üîß Data preprocessing\")\n",
    "    if 'train_df' not in locals() or 'eval_df' not in locals():\n",
    "        missing_vars.append(\"üîÄ Data splitting\")\n",
    "        \n",
    "    if missing_vars:\n",
    "        print(\"Missing steps:\")\n",
    "        for step in missing_vars:\n",
    "            print(f\"  ‚ùå {step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
