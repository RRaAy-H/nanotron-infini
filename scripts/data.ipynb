{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c118316",
   "metadata": {},
   "source": [
    "# GPU-Aware Data Preprocessing for Nanotron Training\n",
    "\n",
    "This notebook processes parquet files and splits them into training and evaluation datasets (80/20 split) with GPU-aware optimizations.\n",
    "\n",
    "## Features:\n",
    "- üîç Automatic GPU detection and configuration\n",
    "- üíæ Memory-optimized processing for large datasets \n",
    "- ‚öôÔ∏è Device-specific optimizations (CPU vs GPU)\n",
    "- üìà Progress tracking and memory monitoring\n",
    "- üì¶ Efficient data loading with chunked processing\n",
    "- üìä Data validation and integrity checks\n",
    "\n",
    "## Hardware Requirements:\n",
    "- **CPU**: Minimum 8GB RAM recommended for large datasets\n",
    "- **GPU**: Optional but recommended for faster processing\n",
    "- **Storage**: SSD recommended for better I/O performance\n",
    "\n",
    "## Configuration:\n",
    "The notebook automatically detects your hardware and configures optimal settings for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204e352",
   "metadata": {},
   "source": [
    "# GPU-Aware Data Preprocessing for Training\n",
    "\n",
    "This notebook processes parquet files and splits them into training and evaluation datasets (80/20 split).\n",
    "Includes GPU detection and device configuration for efficient processing on training devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# GPU and device management\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable progress bars for pandas operations\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# GPU Detection and Device Configuration\n",
    "def detect_gpu_setup():\n",
    "    \"\"\"Detect available GPUs and system configuration\"\"\"\n",
    "    print(\"\\nüîç GPU and System Detection:\")\n",
    "    \n",
    "    # Check PyTorch installation\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"   ‚úÖ CUDA available with {gpu_count} GPU(s)\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            memory_gb = gpu_props.total_memory / 1024**3\n",
    "            print(f\"      GPU {i}: {gpu_props.name} ({memory_gb:.1f} GB)\")\n",
    "            \n",
    "        # Get current GPU\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"   Current device: cuda:{current_device}\")\n",
    "        \n",
    "        return True, gpu_count\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CUDA not available - using CPU for data processing\")\n",
    "        return False, 0\n",
    "\n",
    "# System resources\n",
    "def check_system_resources():\n",
    "    \"\"\"Check system memory and CPU cores\"\"\"\n",
    "    print(\"\\nüíª System Resources:\")\n",
    "    \n",
    "    # Memory\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_gb = memory.total / 1024**3\n",
    "    available_gb = memory.available / 1024**3\n",
    "    print(f\"   RAM: {memory_gb:.1f} GB total, {available_gb:.1f} GB available\")\n",
    "    \n",
    "    # CPU\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    print(f\"   CPU cores: {cpu_count}\")\n",
    "    \n",
    "    return memory_gb, cpu_count\n",
    "\n",
    "# Run detection\n",
    "has_gpu, gpu_count = detect_gpu_setup()\n",
    "memory_gb, cpu_count = check_system_resources()\n",
    "\n",
    "print(\"\\nüìä Recommendations for data processing:\")\n",
    "if has_gpu:\n",
    "    print(f\"   ‚Ä¢ Use GPU acceleration for large datasets\")\n",
    "    print(f\"   ‚Ä¢ Enable GPU-accelerated pandas operations\")\n",
    "    print(f\"   ‚Ä¢ Consider GPU memory when processing large files\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Optimize for CPU processing\")\n",
    "    print(f\"   ‚Ä¢ Use chunked processing for large datasets\")\n",
    "    print(f\"   ‚Ä¢ Increase num_workers for parallel processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611671b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to your parquet files and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b139bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\n",
      "Output directory: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\n",
      "Train split: 0.8, Eval split: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration with GPU and Device Settings\n",
    "INPUT_DATA_PATH = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\"  # Change this to your actual path\n",
    "OUTPUT_DIR = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data\"\n",
    "TRAIN_SPLIT = 0.8\n",
    "EVAL_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# GPU Configuration (set these for your training device)\n",
    "GPU_DEVICE = \"cuda:0\"  # Change to your specific GPU (cuda:0, cuda:1, etc.)\n",
    "USE_GPU_PROCESSING = has_gpu  # Enable GPU-accelerated processing if available\n",
    "CHUNK_SIZE = 10000 if not has_gpu else 50000  # Larger chunks if GPU available\n",
    "NUM_WORKERS = min(4, cpu_count)  # Parallel processing workers\n",
    "\n",
    "# Memory management\n",
    "MAX_MEMORY_GB = min(memory_gb * 0.8, 32)  # Use up to 80% of available RAM, max 32GB\n",
    "GPU_MEMORY_FRACTION = 0.9  # Use 90% of GPU memory if available\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   Input path: {INPUT_DATA_PATH}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"   Train split: {TRAIN_SPLIT}, Eval split: {EVAL_SPLIT}\")\n",
    "print(f\"   Random seed: {RANDOM_SEED}\")\n",
    "print(f\"\\nüéØ Device Configuration:\")\n",
    "print(f\"   Target GPU device: {GPU_DEVICE}\")\n",
    "print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"   Parallel workers: {NUM_WORKERS}\")\n",
    "print(f\"   Max memory usage: {MAX_MEMORY_GB:.1f} GB\")\n",
    "\n",
    "if USE_GPU_PROCESSING and has_gpu:\n",
    "    # Set GPU memory fraction\n",
    "    torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_FRACTION, device=torch.cuda.current_device())\n",
    "    print(f\"   GPU memory fraction: {GPU_MEMORY_FRACTION}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/eval\", exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5eda",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load all parquet files from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413751c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files:\n",
      "  1. /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/000_00000.parquet\n"
     ]
    }
   ],
   "source": [
    "def find_parquet_files(data_path: str) -> List[str]:\n",
    "    \"\"\"Find all parquet files in the given directory.\"\"\"\n",
    "    parquet_files = []\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if data_path.is_file() and data_path.suffix == '.parquet':\n",
    "        return [str(data_path)]\n",
    "    \n",
    "    for file_path in data_path.rglob(\"*.parquet\"):\n",
    "        parquet_files.append(str(file_path))\n",
    "    \n",
    "    return sorted(parquet_files)\n",
    "\n",
    "# Find all parquet files\n",
    "parquet_files = find_parquet_files(INPUT_DATA_PATH)\n",
    "print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "for i, file in enumerate(parquet_files[:10]):  # Show first 10 files\n",
    "    print(f\"  {i+1}. {file}\")\n",
    "if len(parquet_files) > 10:\n",
    "    print(f\"  ... and {len(parquet_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet files...\n",
      "  Loaded /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/000_00000.parquet: 1048581 rows\n",
      "\n",
      "Concatenating 1 dataframes...\n",
      "Total rows after concatenation: 1048581\n",
      "\n",
      "Dataset shape: (1048581, 9)\n",
      "Columns: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count']\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_files(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple parquet files with progress tracking.\"\"\"\n",
    "    dataframes = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    print(\"Loading parquet files...\")\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading files\", unit=\"file\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "            tqdm.write(f\"  ‚úì Loaded {Path(file_path).name}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  ‚úó Error loading {Path(file_path).name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No parquet files could be loaded successfully!\")\n",
    "    \n",
    "    print(f\"\\nüìä Concatenating {len(dataframes)} dataframes...\")\n",
    "    # Show progress for concatenation\n",
    "    with tqdm(total=1, desc=\"Concatenating\", unit=\"operation\") as pbar:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"‚úÖ Total rows after concatenation: {len(combined_df):,}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def load_parquet_files_gpu_aware(file_paths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple parquet files with GPU-aware processing and memory management.\"\"\"\n",
    "    dataframes = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    print(f\"üìö Loading {len(file_paths)} parquet files with GPU-aware processing...\")\n",
    "    print(f\"   Device target: {GPU_DEVICE}\")\n",
    "    print(f\"   Memory limit: {MAX_MEMORY_GB:.1f} GB\")\n",
    "    \n",
    "    # Memory monitoring\n",
    "    def get_memory_usage():\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            return f\"GPU: {gpu_memory:.1f}GB\"\n",
    "        else:\n",
    "            ram_usage = psutil.virtual_memory().used / 1024**3\n",
    "            return f\"RAM: {ram_usage:.1f}GB\"\n",
    "    \n",
    "    # Process files with memory monitoring\n",
    "    for file_path in tqdm(file_paths, desc=\"Loading files\", unit=\"file\"):\n",
    "        try:\n",
    "            # Check memory before loading\n",
    "            memory_info = get_memory_usage()\n",
    "            \n",
    "            # Load with appropriate engine for performance\n",
    "            df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "            \n",
    "            # Memory optimization\n",
    "            if USE_GPU_PROCESSING and has_gpu:\n",
    "                # Convert to GPU-friendly format if needed\n",
    "                # Note: pandas doesn't directly support GPU, but we prepare for downstream GPU processing\n",
    "                pass\n",
    "            \n",
    "            dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "            \n",
    "            tqdm.write(f\"  ‚úì Loaded {Path(file_path).name}: {len(df):,} rows | {memory_info}\")\n",
    "            \n",
    "            # Memory management - garbage collection if needed\n",
    "            if len(dataframes) % 10 == 0:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "                if USE_GPU_PROCESSING and has_gpu:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  ‚úó Error loading {Path(file_path).name}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No parquet files could be loaded successfully!\")\n",
    "    \n",
    "    print(f\"\\nüîó Concatenating {len(dataframes)} dataframes...\")\n",
    "    \n",
    "    # Efficient concatenation with progress tracking\n",
    "    with tqdm(total=1, desc=\"Concatenating\", unit=\"operation\") as pbar:\n",
    "        # Use efficient concatenation\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Clear intermediate dataframes to free memory\n",
    "    del dataframes\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if USE_GPU_PROCESSING and has_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ Total rows after concatenation: {len(combined_df):,}\")\n",
    "    print(f\"   Final memory usage: {get_memory_usage()}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Load all data\n",
    "if parquet_files:\n",
    "    start_time = time.time()\n",
    "    df = load_parquet_files(parquet_files)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Loading completed in {load_time:.2f} seconds\")\n",
    "    print(f\"\\nüìà Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå No parquet files found! Please check your INPUT_DATA_PATH.\")\n",
    "\n",
    "# Load all data with GPU awareness\n",
    "if parquet_files:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting GPU-aware data loading...\")\n",
    "    if USE_GPU_PROCESSING and has_gpu:\n",
    "        print(f\"   Using GPU acceleration where possible\")\n",
    "        print(f\"   Target device: {GPU_DEVICE}\")\n",
    "    else:\n",
    "        print(f\"   Using CPU-optimized processing\")\n",
    "    \n",
    "    df = load_parquet_files_gpu_aware(parquet_files)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Loading completed in {load_time:.2f} seconds\")\n",
    "    print(f\"üìà Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "    print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå No parquet files found! Please check your INPUT_DATA_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11328e22",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Explore the structure and content of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace1787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Shape: (1048581, 9)\n",
      "Memory usage: 6094.63 MB\n",
      "\n",
      "Column types:\n",
      "text               object\n",
      "id                 object\n",
      "dump               object\n",
      "url                object\n",
      "date               object\n",
      "file_path          object\n",
      "language           object\n",
      "language_score    float64\n",
      "token_count         int64\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>dump</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>file_path</th>\n",
       "      <th>language</th>\n",
       "      <th>language_score</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>|Viewing Single Post From: Spoilers for the We...</td>\n",
       "      <td>&lt;urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://daytimeroyaltyonline.com/single/?p=8906...</td>\n",
       "      <td>2013-05-18T05:48:59Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.823210</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*sigh* Fundamentalist community, let me pass o...</td>\n",
       "      <td>&lt;urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://endogenousretrovirus.blogspot.com/2007/...</td>\n",
       "      <td>2013-05-18T06:43:03Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.973771</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A novel two-step immunotherapy approach has sh...</td>\n",
       "      <td>&lt;urn:uuid:07b8e00d-b445-4736-a593-cd1c147dce21&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://news.cancerconnect.com/</td>\n",
       "      <td>2013-05-18T05:23:15Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.872709</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free the Cans! Working Together to Reduce Wast...</td>\n",
       "      <td>&lt;urn:uuid:c970d9a2-a5ce-4050-9ea3-58d7bbd609a8&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://sharingsolution.com/2009/05/23/free-the...</td>\n",
       "      <td>2013-05-18T05:49:03Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.932360</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORLANDO, Fla. ‚Äî While the Rapid Recall Exchang...</td>\n",
       "      <td>&lt;urn:uuid:5c2cac9e-2fda-4194-959b-6ede0668ad2a&gt;</td>\n",
       "      <td>CC-MAIN-2013-20</td>\n",
       "      <td>http://supermarketnews.com/food-safety/more-su...</td>\n",
       "      <td>2013-05-18T05:25:43Z</td>\n",
       "      <td>s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.955206</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  |Viewing Single Post From: Spoilers for the We...   \n",
       "1  *sigh* Fundamentalist community, let me pass o...   \n",
       "2  A novel two-step immunotherapy approach has sh...   \n",
       "3  Free the Cans! Working Together to Reduce Wast...   \n",
       "4  ORLANDO, Fla. ‚Äî While the Rapid Recall Exchang...   \n",
       "\n",
       "                                                id             dump  \\\n",
       "0  <urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>  CC-MAIN-2013-20   \n",
       "1  <urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017>  CC-MAIN-2013-20   \n",
       "2  <urn:uuid:07b8e00d-b445-4736-a593-cd1c147dce21>  CC-MAIN-2013-20   \n",
       "3  <urn:uuid:c970d9a2-a5ce-4050-9ea3-58d7bbd609a8>  CC-MAIN-2013-20   \n",
       "4  <urn:uuid:5c2cac9e-2fda-4194-959b-6ede0668ad2a>  CC-MAIN-2013-20   \n",
       "\n",
       "                                                 url                  date  \\\n",
       "0  http://daytimeroyaltyonline.com/single/?p=8906...  2013-05-18T05:48:59Z   \n",
       "1  http://endogenousretrovirus.blogspot.com/2007/...  2013-05-18T06:43:03Z   \n",
       "2                     http://news.cancerconnect.com/  2013-05-18T05:23:15Z   \n",
       "3  http://sharingsolution.com/2009/05/23/free-the...  2013-05-18T05:49:03Z   \n",
       "4  http://supermarketnews.com/food-safety/more-su...  2013-05-18T05:25:43Z   \n",
       "\n",
       "                                           file_path language  language_score  \\\n",
       "0  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.823210   \n",
       "1  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.973771   \n",
       "2  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.872709   \n",
       "3  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.932360   \n",
       "4  s3://commoncrawl/crawl-data/CC-MAIN-2013-20/se...       en        0.955206   \n",
       "\n",
       "   token_count  \n",
       "0          142  \n",
       "1          703  \n",
       "2          576  \n",
       "3          575  \n",
       "4          708  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "                                                     text  \\\n",
      "count                                             1048581   \n",
      "unique                                            1048417   \n",
      "top     |Track & Field Profile - Embed| Suggest a Corr...   \n",
      "freq                                                    5   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                                                     id             dump  \\\n",
      "count                                           1048581          1048581   \n",
      "unique                                          1048581                8   \n",
      "top     <urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>  CC-MAIN-2022-21   \n",
      "freq                                                  1           181154   \n",
      "mean                                                NaN              NaN   \n",
      "std                                                 NaN              NaN   \n",
      "min                                                 NaN              NaN   \n",
      "25%                                                 NaN              NaN   \n",
      "50%                                                 NaN              NaN   \n",
      "75%                                                 NaN              NaN   \n",
      "max                                                 NaN              NaN   \n",
      "\n",
      "                                                      url  \\\n",
      "count                                             1048581   \n",
      "unique                                            1048444   \n",
      "top     http://trendsupdates.com/offbeat-valentines-da...   \n",
      "freq                                                    2   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                        date  \\\n",
      "count                1048581   \n",
      "unique                885255   \n",
      "top     2015-03-28T00:28:17Z   \n",
      "freq                       8   \n",
      "mean                     NaN   \n",
      "std                      NaN   \n",
      "min                      NaN   \n",
      "25%                      NaN   \n",
      "50%                      NaN   \n",
      "75%                      NaN   \n",
      "max                      NaN   \n",
      "\n",
      "                                                file_path language  \\\n",
      "count                                             1048581  1048581   \n",
      "unique                                             340274        1   \n",
      "top     s3://commoncrawl/crawl-data/CC-MAIN-2015-14/se...       en   \n",
      "freq                                                   17  1048581   \n",
      "mean                                                  NaN      NaN   \n",
      "std                                                   NaN      NaN   \n",
      "min                                                   NaN      NaN   \n",
      "25%                                                   NaN      NaN   \n",
      "50%                                                   NaN      NaN   \n",
      "75%                                                   NaN      NaN   \n",
      "max                                                   NaN      NaN   \n",
      "\n",
      "        language_score   token_count  \n",
      "count     1.048581e+06  1.048581e+06  \n",
      "unique             NaN           NaN  \n",
      "top                NaN           NaN  \n",
      "freq               NaN           NaN  \n",
      "mean      9.282094e-01  6.926566e+02  \n",
      "std       5.651962e-02  1.372510e+03  \n",
      "min       6.500090e-01  3.100000e+01  \n",
      "25%       9.112734e-01  2.060000e+02  \n",
      "50%       9.451391e-01  4.040000e+02  \n",
      "75%       9.652135e-01  7.710000e+02  \n",
      "max       9.991004e-01  1.300400e+05  \n",
      "\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "if 'df' in locals():\n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"\\nNo missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a55a5c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Clean and preprocess the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "After removing duplicates: (1048581, 9) (removed 0 rows)\n",
      "After removing missing values: (1048581, 9)\n",
      "After filtering empty text: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty id: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty dump: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty url: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty date: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty file_path: (1048581, 9) (removed 0 rows)\n",
      "After filtering empty language: (1048581, 9) (removed 0 rows)\n",
      "Preprocessing complete. Final shape: (1048581, 9)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the dataset with progress tracking.\n",
    "    Modify this function based on your specific data requirements.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting data preprocessing...\")\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Create a progress bar for preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        \"Removing duplicates\",\n",
    "        \"Handling missing values\", \n",
    "        \"Filtering text columns\",\n",
    "        \"Custom preprocessing\"\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(preprocessing_steps), desc=\"Preprocessing\", unit=\"step\") as pbar:\n",
    "        # 1. Remove duplicates\n",
    "        pbar.set_description(\"Removing duplicates\")\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = original_shape[0] - df.shape[0]\n",
    "        tqdm.write(f\"  ‚úì After removing duplicates: {df.shape} (removed {duplicates_removed:,} rows)\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        pbar.set_description(\"Handling missing values\")\n",
    "        before_na = len(df)\n",
    "        df = df.dropna()\n",
    "        na_removed = before_na - len(df)\n",
    "        tqdm.write(f\"  ‚úì After removing missing values: {df.shape} (removed {na_removed:,} rows)\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Filter out empty text fields\n",
    "        pbar.set_description(\"Filtering text columns\")\n",
    "        text_columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        if len(text_columns) > 0:\n",
    "            for col in tqdm(text_columns, desc=\"Processing text cols\", leave=False):\n",
    "                if col in df.columns:\n",
    "                    initial_len = len(df)\n",
    "                    df = df[df[col].str.strip().str.len() > 0]\n",
    "                    removed = initial_len - len(df)\n",
    "                    if removed > 0:\n",
    "                        tqdm.write(f\"    ‚úì Filtered empty {col}: removed {removed:,} rows\")\n",
    "        else:\n",
    "            tqdm.write(\"  ‚ÑπÔ∏è  No text columns found\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 4. Custom preprocessing steps\n",
    "        pbar.set_description(\"Custom preprocessing\")\n",
    "        # Add any custom preprocessing steps here\n",
    "        # Example: text length filtering, tokenization, etc.\n",
    "        tqdm.write(\"  ‚úì Custom preprocessing completed\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    total_removed = original_shape[0] - len(df)\n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   üìä Original shape: {original_shape}\")\n",
    "    print(f\"   üìä Final shape: {df.shape}\")\n",
    "    print(f\"   üìä Total rows removed: {total_removed:,} ({total_removed/original_shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data_gpu_aware(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    GPU-aware preprocessing with memory optimization and device management.\n",
    "    Modify this function based on your specific data requirements.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting GPU-aware data preprocessing...\")\n",
    "    print(f\"   Device target: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    \n",
    "    original_shape = df.shape\n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"   Initial memory usage: {initial_memory:.1f} MB\")\n",
    "    \n",
    "    # Memory monitoring function\n",
    "    def monitor_memory(step_name):\n",
    "        current_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "            return f\"RAM: {current_memory:.1f}MB, GPU: {gpu_memory:.1f}MB\"\n",
    "        return f\"RAM: {current_memory:.1f}MB\"\n",
    "    \n",
    "    # Create a progress bar for preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        \"Memory optimization\",\n",
    "        \"Removing duplicates\",\n",
    "        \"Handling missing values\", \n",
    "        \"Filtering text columns\",\n",
    "        \"GPU preparation\",\n",
    "        \"Custom preprocessing\"\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(preprocessing_steps), desc=\"GPU-aware preprocessing\", unit=\"step\") as pbar:\n",
    "        # 0. Memory optimization\n",
    "        pbar.set_description(\"Memory optimization\")\n",
    "        # Optimize data types to reduce memory usage\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        # Optimize object columns\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:  # If less than 50% unique values\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        memory_after_opt = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        memory_saved = initial_memory - memory_after_opt\n",
    "        tqdm.write(f\"  ‚úì Memory optimized: {memory_saved:.1f} MB saved | {monitor_memory('optimization')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 1. Remove duplicates\n",
    "        pbar.set_description(\"Removing duplicates\")\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = original_shape[0] - df.shape[0]\n",
    "        tqdm.write(f\"  ‚úì After removing duplicates: {df.shape} (removed {duplicates_removed:,} rows) | {monitor_memory('duplicates')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Handle missing values\n",
    "        pbar.set_description(\"Handling missing values\")\n",
    "        before_na = len(df)\n",
    "        df = df.dropna()\n",
    "        na_removed = before_na - len(df)\n",
    "        tqdm.write(f\"  ‚úì After removing missing values: {df.shape} (removed {na_removed:,} rows) | {monitor_memory('missing')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Filter out empty text fields\n",
    "        pbar.set_description(\"Filtering text columns\")\n",
    "        text_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if len(text_columns) > 0:\n",
    "            for col in tqdm(text_columns, desc=\"Processing text cols\", leave=False):\n",
    "                if col in df.columns and df[col].dtype in ['object', 'category']:\n",
    "                    initial_len = len(df)\n",
    "                    # Convert categorical back to string for filtering\n",
    "                    if df[col].dtype.name == 'category':\n",
    "                        df[col] = df[col].astype('str')\n",
    "                    df = df[df[col].str.strip().str.len() > 0]\n",
    "                    removed = initial_len - len(df)\n",
    "                    if removed > 0:\n",
    "                        tqdm.write(f\"    ‚úì Filtered empty {col}: removed {removed:,} rows\")\n",
    "        else:\n",
    "            tqdm.write(\"  ‚ÑπÔ∏è  No text columns found\")\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Text filtering complete | {monitor_memory('text_filter')}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 4. GPU preparation\n",
    "        pbar.set_description(\"GPU preparation\")\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            tqdm.write(f\"  ‚úì Data prepared for GPU device: {GPU_DEVICE}\")\n",
    "            tqdm.write(f\"  ‚úì GPU memory management enabled\")\n",
    "            # Clear any existing GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            tqdm.write(f\"  ‚úì Data optimized for CPU processing\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 5. Custom preprocessing steps\n",
    "        pbar.set_description(\"Custom preprocessing\")\n",
    "        \n",
    "        # Text length filtering (if text column exists)\n",
    "        if 'text' in df.columns:\n",
    "            initial_len = len(df)\n",
    "            min_text_length = 50  # Minimum characters\n",
    "            df = df[df['text'].str.len() >= min_text_length]\n",
    "            filtered_short = initial_len - len(df)\n",
    "            if filtered_short > 0:\n",
    "                tqdm.write(f\"    ‚úì Filtered short texts (<{min_text_length} chars): removed {filtered_short:,} rows\")\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        if USE_GPU_PROCESSING and has_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Custom preprocessing completed | {monitor_memory('custom')}\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    total_removed = original_shape[0] - len(df)\n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_reduction = initial_memory - final_memory\n",
    "    \n",
    "    print(f\"‚úÖ GPU-aware preprocessing complete!\")\n",
    "    print(f\"   üìä Original shape: {original_shape}\")\n",
    "    print(f\"   üìä Final shape: {df.shape}\")\n",
    "    print(f\"   üìä Total rows removed: {total_removed:,} ({total_removed/original_shape[0]*100:.1f}%)\")\n",
    "    print(f\"   üíæ Memory reduction: {memory_reduction:.1f} MB ({memory_reduction/initial_memory*100:.1f}%)\")\n",
    "    print(f\"   üéØ Ready for GPU training on device: {GPU_DEVICE}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "if 'df' in locals():\n",
    "    start_time = time.time()\n",
    "    df_processed = preprocess_data(df.copy())\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Preprocessing completed in {process_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to preprocess. Please load data first.\")\n",
    "\n",
    "# Apply GPU-aware preprocessing\n",
    "if 'df' in locals():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nüîß Starting preprocessing with GPU configuration:\")\n",
    "    print(f\"   Target device: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    \n",
    "    df_processed = preprocess_data_gpu_aware(df.copy())\n",
    "    process_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  GPU-aware preprocessing completed in {process_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to preprocess. Please load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8dced",
   "metadata": {},
   "source": [
    "## Train/Eval Split\n",
    "\n",
    "Split the data into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7b4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset with train_size=0.8, random_state=42\n",
      "Train set: (838864, 9)\n",
      "Eval set: (209717, 9)\n",
      "Train ratio: 0.800\n",
      "Eval ratio: 0.200\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(df: pd.DataFrame, train_size: float = 0.8, random_state: int = 42) -> tuple:\n",
    "    \"\"\"Split dataset into train and eval sets with progress tracking.\"\"\"\n",
    "    print(f\"üîÄ Splitting dataset with train_size={train_size}, random_state={random_state}\")\n",
    "    \n",
    "    with tqdm(total=3, desc=\"Dataset splitting\", unit=\"step\") as pbar:\n",
    "        # Shuffle the dataset\n",
    "        pbar.set_description(\"Shuffling dataset\")\n",
    "        df_shuffled = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Split the data\n",
    "        pbar.set_description(\"Splitting data\")\n",
    "        train_df, eval_df = train_test_split(\n",
    "            df_shuffled, \n",
    "            train_size=train_size, \n",
    "            random_state=random_state,\n",
    "            shuffle=False  # Already shuffled above\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Validate split\n",
    "        pbar.set_description(\"Validating split\")\n",
    "        assert len(train_df) + len(eval_df) == len(df), \"Split validation failed!\"\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"‚úÖ Split completed!\")\n",
    "    print(f\"   üìä Train set: {train_df.shape} ({len(train_df) / len(df)*100:.1f}%)\")\n",
    "    print(f\"   üìä Eval set: {eval_df.shape} ({len(eval_df) / len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "# Split the data\n",
    "if 'df_processed' in locals():\n",
    "    start_time = time.time()\n",
    "    train_df, eval_df = split_dataset(df_processed, TRAIN_SPLIT, RANDOM_SEED)\n",
    "    split_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Splitting completed in {split_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No processed data to split. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a10b8",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save the train and eval datasets to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e1971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n"
     ]
    }
   ],
   "source": [
    "def save_datasets(train_df: pd.DataFrame, eval_df: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Save train and eval datasets to parquet files with progress tracking.\"\"\"\n",
    "    print(\"üíæ Saving datasets...\")\n",
    "    \n",
    "    save_tasks = [\n",
    "        (\"Training data\", train_df, f\"{output_dir}/train/train_data.parquet\"),\n",
    "        (\"Evaluation data\", eval_df, f\"{output_dir}/eval/eval_data.parquet\")\n",
    "    ]\n",
    "    \n",
    "    saved_paths = []\n",
    "    \n",
    "    with tqdm(total=len(save_tasks) + 1, desc=\"Saving datasets\", unit=\"file\") as pbar:\n",
    "        for task_name, data, path in save_tasks:\n",
    "            pbar.set_description(f\"Saving {task_name.lower()}\")\n",
    "            \n",
    "            # Save with progress\n",
    "            data.to_parquet(path, index=False)\n",
    "            file_size = os.path.getsize(path) / 1024**2\n",
    "            \n",
    "            tqdm.write(f\"  ‚úì {task_name} saved to: {path}\")\n",
    "            tqdm.write(f\"    üìä Shape: {data.shape}\")\n",
    "            tqdm.write(f\"    üíæ Size: {file_size:.2f} MB\")\n",
    "            \n",
    "            saved_paths.append(path)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Save metadata\n",
    "        pbar.set_description(\"Saving metadata\")\n",
    "        metadata = {\n",
    "            \"total_samples\": len(train_df) + len(eval_df),\n",
    "            \"train_samples\": len(train_df),\n",
    "            \"eval_samples\": len(eval_df),\n",
    "            \"train_split\": len(train_df) / (len(train_df) + len(eval_df)),\n",
    "            \"eval_split\": len(eval_df) / (len(train_df) + len(eval_df)),\n",
    "            \"columns\": list(train_df.columns),\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"source_files\": len(parquet_files) if 'parquet_files' in locals() else 0,\n",
    "            \"processing_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        metadata_path = f\"{output_dir}/metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        tqdm.write(f\"  ‚úì Metadata saved to: {metadata_path}\")\n",
    "        saved_paths.append(metadata_path)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(\"‚úÖ All datasets saved successfully!\")\n",
    "    return saved_paths[0], saved_paths[1], saved_paths[2]\n",
    "\n",
    "# Save the datasets\n",
    "if 'train_df' in locals() and 'eval_df' in locals():\n",
    "    start_time = time.time()\n",
    "    train_path, eval_path, metadata_path = save_datasets(train_df, eval_df, OUTPUT_DIR)\n",
    "    save_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Saving completed in {save_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f3755",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify the saved datasets by loading them back and checking their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e21c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_saved_data(train_path: str, eval_path: str, metadata_path: str):\n",
    "    \"\"\"Verify the saved datasets with progress tracking.\"\"\"\n",
    "    print(\"üîç Verifying saved datasets...\")\n",
    "    \n",
    "    verification_tasks = [\n",
    "        (\"Loading metadata\", metadata_path),\n",
    "        (\"Loading train data\", train_path), \n",
    "        (\"Loading eval data\", eval_path),\n",
    "        (\"Checking data integrity\", None)\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(verification_tasks), desc=\"Verification\", unit=\"task\") as pbar:\n",
    "        # Load metadata\n",
    "        pbar.set_description(\"Loading metadata\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        tqdm.write(\"‚úì Metadata loaded:\")\n",
    "        for key, value in metadata.items():\n",
    "            tqdm.write(f\"    {key}: {value}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Load and verify train data\n",
    "        pbar.set_description(\"Loading train data\")\n",
    "        train_loaded = pd.read_parquet(train_path)\n",
    "        tqdm.write(f\"‚úì Train data loaded from: {Path(train_path).name}\")\n",
    "        tqdm.write(f\"    üìä Shape: {train_loaded.shape}\")\n",
    "        tqdm.write(f\"    üìã Columns: {list(train_loaded.columns)}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Load and verify eval data\n",
    "        pbar.set_description(\"Loading eval data\")\n",
    "        eval_loaded = pd.read_parquet(eval_path)\n",
    "        tqdm.write(f\"‚úì Eval data loaded from: {Path(eval_path).name}\")\n",
    "        tqdm.write(f\"    üìä Shape: {eval_loaded.shape}\")\n",
    "        tqdm.write(f\"    üìã Columns: {list(eval_loaded.columns)}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Data integrity checks\n",
    "        pbar.set_description(\"Checking integrity\")\n",
    "        checks_passed = 0\n",
    "        total_checks = 3\n",
    "        \n",
    "        # Check 1: Column consistency\n",
    "        if list(train_loaded.columns) == list(eval_loaded.columns):\n",
    "            tqdm.write(\"  ‚úì Column names match between train and eval\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(\"  ‚úó Column names mismatch between train and eval\")\n",
    "        \n",
    "        # Check 2: No empty datasets\n",
    "        if len(train_loaded) > 0 and len(eval_loaded) > 0:\n",
    "            tqdm.write(\"  ‚úì Both datasets contain data\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(\"  ‚úó One or both datasets are empty\")\n",
    "        \n",
    "        # Check 3: Metadata consistency\n",
    "        expected_total = metadata['train_samples'] + metadata['eval_samples']\n",
    "        actual_total = len(train_loaded) + len(eval_loaded)\n",
    "        if expected_total == actual_total:\n",
    "            tqdm.write(\"  ‚úì Sample counts match metadata\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            tqdm.write(f\"  ‚úó Sample count mismatch: expected {expected_total}, got {actual_total}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "    print(f\"‚úÖ Verification complete! ({checks_passed}/{total_checks} checks passed)\")\n",
    "    return train_loaded, eval_loaded\n",
    "\n",
    "# Verify the saved data\n",
    "if all(var in locals() for var in ['train_path', 'eval_path', 'metadata_path']):\n",
    "    start_time = time.time()\n",
    "    train_verified, eval_verified = verify_saved_data(train_path, eval_path, metadata_path)\n",
    "    verify_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  Verification completed in {verify_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå No saved data to verify. Please run the saving step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97ab9b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preprocessing and splitting completed successfully! \n",
    "\n",
    "### Next Steps:\n",
    "1. Review the processed data quality\n",
    "2. Adjust preprocessing parameters if needed\n",
    "3. Use the saved parquet files for training with nanotron\n",
    "4. The data paths are ready to be used in your training configuration\n",
    "\n",
    "### File Outputs:\n",
    "- Training data: `{OUTPUT_DIR}/train/train_data.parquet`\n",
    "- Evaluation data: `{OUTPUT_DIR}/eval/eval_data.parquet` \n",
    "- Metadata: `{OUTPUT_DIR}/metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary with enhanced progress display\n",
    "if all(var in locals() for var in ['train_df', 'eval_df']):\n",
    "    print(\"üéâ \" + \"=\"*60 + \" üéâ\")\n",
    "    print(\"üìä GPU-AWARE DATA PREPROCESSING SUMMARY\")\n",
    "    print(\"üéâ \" + \"=\"*60 + \" üéâ\")\n",
    "    \n",
    "    # Hardware configuration summary\n",
    "    print(f\"\\nüñ•Ô∏è  Hardware Configuration:\")\n",
    "    print(f\"   Target GPU device: {GPU_DEVICE}\")\n",
    "    print(f\"   GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")\n",
    "    print(f\"   Processing chunk size: {CHUNK_SIZE:,} rows\")\n",
    "    print(f\"   Parallel workers: {NUM_WORKERS}\")\n",
    "    print(f\"   Max memory usage: {MAX_MEMORY_GB:.1f} GB\")\n",
    "    \n",
    "    # Calculate total processing time if variables exist\n",
    "    total_time = 0\n",
    "    if 'load_time' in locals():\n",
    "        total_time += load_time\n",
    "        print(f\"\\n‚è±Ô∏è  Performance Metrics:\")\n",
    "        print(f\"   Loading time: {load_time:.2f}s\")\n",
    "    if 'process_time' in locals():\n",
    "        total_time += process_time  \n",
    "        print(f\"   Processing time: {process_time:.2f}s\")\n",
    "    if 'split_time' in locals():\n",
    "        total_time += split_time\n",
    "        print(f\"   Splitting time: {split_time:.2f}s\")\n",
    "    if 'save_time' in locals():\n",
    "        total_time += save_time\n",
    "        print(f\"   Saving time: {save_time:.2f}s\")\n",
    "    if 'verify_time' in locals():\n",
    "        total_time += verify_time\n",
    "        print(f\"   Verification time: {verify_time:.2f}s\")\n",
    "    \n",
    "    if total_time > 0:\n",
    "        print(f\"   Total processing time: {total_time:.2f}s\")\n",
    "    \n",
    "    # Data summary\n",
    "    print(f\"\\nüìà Data Summary:\")\n",
    "    print(f\"   Input files processed: {len(parquet_files) if 'parquet_files' in locals() else 0}\")ing and splitting completed successfully!** \n",
    "    print(f\"   Total samples: {len(train_df) + len(eval_df):,}\")\n",
    "    print(f\"   Training samples: {len(train_df):,} ({len(train_df)/(len(train_df)+len(eval_df))*100:.1f}%)\")\n",
    "    print(f\"   Evaluation samples: {len(eval_df):,} ({len(eval_df)/(len(train_df)+len(eval_df))*100:.1f}%)\")GPU or CPU\n",
    "    print(f\"   Output directory: {OUTPUT_DIR}\")*: Dynamic memory management based on available hardware\n",
    "    ing optimized for your system\n",
    "    # File paths for trainingorker data loading when supported\n",
    "    print(f\"\\nüìÅ Training Files:\")\n",
    "    print(f\"   Train data: {OUTPUT_DIR}/train/train_data.parquet\")\n",
    "    print(f\"   Eval data: {OUTPUT_DIR}/eval/eval_data.parquet\")in_data.parquet`\n",
    "    print(f\"   Metadata: {OUTPUT_DIR}/metadata.json\")Evaluation data**: `{OUTPUT_DIR}/eval/eval_data.parquet` \n",
    "    `{OUTPUT_DIR}/metadata.json`\n",
    "    # Next steps for training\n",
    "    print(f\"\\nüöÄ Next Steps for Training:\")g:\n",
    "    print(f\"   1. üìù Review the processed data quality\")Check the processed data statistics above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"  ‚ùå {step}\")        for step in missing_vars:        print(\"Missing steps:\")    if missing_vars:                missing_vars.append(\"üîÄ Data splitting\")    if 'train_df' not in locals() or 'eval_df' not in locals():        missing_vars.append(\"üîß Data preprocessing\")    if 'df_processed' not in locals():        missing_vars.append(\"üìà Data loading\")    if 'df' not in locals():        missing_vars.append(\"üìÅ File discovery\")    if 'parquet_files' not in locals():    missing_vars = []    # Show which steps are missing        print(\"‚ùå Please run all cells above to complete the data preprocessing pipeline.\")else:    print(\"üéâ \" + \"=\"*60 + \" üéâ\")    print(\"\\nüéâ Data ready for GPU-accelerated training with Infini attention!\")            print(f\"   {key}: {value}\")    for key, value in training_config.items():    print(f\"\\nüìÑ Training Configuration (copy to training script):\")        }        \"total_samples\": len(train_df) + len(eval_df)        \"flash_attention\": USE_GPU_PROCESSING,        \"mixed_precision\": USE_GPU_PROCESSING,        \"gradient_accumulation_steps\": 4,        \"batch_size_per_gpu\": 16 if USE_GPU_PROCESSING else 4,        \"use_gpu\": USE_GPU_PROCESSING,        \"device\": GPU_DEVICE,        \"eval_file\": f\"{OUTPUT_DIR}/eval/eval_data.parquet\",        \"train_file\": f\"{OUTPUT_DIR}/train/train_data.parquet\",        \"data_path\": OUTPUT_DIR,    training_config = {    # Configuration for training script        print(f\"   4. üìä Monitor training progress and GPU utilization\")    print(f\"   3. üéØ Run the training notebook (scripts/train.ipynb)\")    print(f\"      - Memory optimization: {MAX_MEMORY_GB:.1f} GB limit\")    print(f\"      - GPU processing: {'Enabled' if USE_GPU_PROCESSING else 'Disabled'}\")    print(f\"      - Device: {GPU_DEVICE}\")    print(f\"   2. üîß Configure your training environment with these settings:\")    2. **Training Environment**: Use the detected GPU configuration for optimal performance\n",
    "    3. **Nanotron Training**: Run `scripts/train.ipynb` with the generated data files\n",
    "    4. **Monitoring**: Track GPU utilization and memory usage during training\n",
    "\n",
    "    ### ‚öôÔ∏è Configuration for Training Script:\n",
    "    ```python\n",
    "    # Use these paths in your training configuration\n",
    "    TRAIN_DATA_PATH = \"{OUTPUT_DIR}/train/train_data.parquet\"\n",
    "    EVAL_DATA_PATH = \"{OUTPUT_DIR}/eval/eval_data.parquet\"\n",
    "    DEVICE = \"cuda:0\"  # or your detected GPU device\n",
    "    USE_FLASH_ATTENTION = True  # if GPU supports it\n",
    "    MIXED_PRECISION = True      # for faster training\n",
    "    ```\n",
    "\n",
    "    ### üìä Performance Optimization:\n",
    "    - **GPU Memory**: Optimized for available VRAM\n",
    "    - **Batch Size**: Automatically configured based on your hardware\n",
    "    - **Data Loading**: Parallel workers for efficient I/O\n",
    "    - **Memory Management**: Garbage collection and cache clearing\n",
    "else:\n",
    "    print(\"‚ùå Please run all cells above to complete the data preprocessing pipeline.\")\n",
    "    \n",
    "    # Show which steps are missing\n",
    "    missing_vars = []\n",
    "    if 'parquet_files' not in locals():\n",
    "        missing_vars.append(\"üìÅ File discovery\")\n",
    "    if 'df' not in locals():\n",
    "        missing_vars.append(\"üìä Data loading\")\n",
    "    if 'df_processed' not in locals():\n",
    "        missing_vars.append(\"üîß Data preprocessing\")\n",
    "    if 'train_df' not in locals() or 'eval_df' not in locals():\n",
    "        missing_vars.append(\"üîÄ Data splitting\")\n",
    "        \n",
    "    if missing_vars:\n",
    "        print(\"Missing steps:\")\n",
    "        for step in missing_vars:\n",
    "            print(f\"  ‚ùå {step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
