{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fbdcb8",
   "metadata": {},
   "source": [
    "# Training 200M LLaMA Model with Infini-Attention using Nanotron\n",
    "\n",
    "This notebook demonstrates how to train a 200M parameter LLaMA model with Infini-Attention using nanotron's distributed trainer and dataloader with your preprocessed data.\n",
    "\n",
    "Infini-Attention extends the LLaMA model with infinite context capabilities by using compressive memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package if it's not already installed\"\"\"\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"‚úÖ {package_name} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"‚úÖ {package_name} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    \"pyyaml\",  # For YAML configuration files\n",
    "    \"psutil\",  # For system monitoring\n",
    "    \"tqdm\",    # For progress bars\n",
    "    \"wandb\",   # For experiment tracking (optional)\n",
    "]\n",
    "\n",
    "print(\"üîß Checking and installing required packages...\")\n",
    "for package in required_packages:\n",
    "    if package == \"pyyaml\":\n",
    "        install_package(\"yaml\")  # pyyaml imports as yaml\n",
    "    else:\n",
    "        install_package(package)\n",
    "\n",
    "print(\"\\nüìö Importing libraries...\")\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Optional\n",
    "import logging\n",
    "import psutil\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "\n",
    "# GPU and Device Detection\n",
    "def detect_training_hardware():\n",
    "    \"\"\"Detect GPU setup and configure for training\"\"\"\n",
    "    print(\"\\nüîç Hardware Detection for Training:\")\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"   ‚úÖ CUDA available with {gpu_count} GPU(s)\")\n",
    "        \n",
    "        flash_attention_gpus = []\n",
    "        for i in range(gpu_count):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            memory_gb = gpu_props.total_memory / 1024**3\n",
    "            print(f\"      GPU {i}: {gpu_props.name} ({memory_gb:.1f} GB)\")\n",
    "            \n",
    "            # Check for Ampere architecture (A100, RTX 30XX series, etc.) for Flash Attention\n",
    "            is_ampere = gpu_props.major >= 8\n",
    "            flash_attention_supported = is_ampere and memory_gb >= 8\n",
    "            print(f\"         Compute capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "            print(f\"         Flash Attention supported: {'‚úÖ' if flash_attention_supported else '‚ùå'}\")\n",
    "            \n",
    "            if flash_attention_supported:\n",
    "                flash_attention_gpus.append(i)\n",
    "            \n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"   Current device: cuda:{current_device}\")\n",
    "        print(f\"   Flash Attention available on GPUs: {flash_attention_gpus}\")\n",
    "        return True, gpu_count, current_device, len(flash_attention_gpus) > 0\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CUDA not available - training will use CPU (not recommended)\")\n",
    "        return False, 0, None, False\n",
    "\n",
    "def check_flash_attention():\n",
    "    \"\"\"Check if Flash Attention is available\"\"\"\n",
    "    try:\n",
    "        import flash_attn\n",
    "        print(\"   ‚úÖ Flash Attention is installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è  Flash Attention not installed\")\n",
    "        print(\"      Install with: pip install flash-attn --no-build-isolation\")\n",
    "        return False\n",
    "\n",
    "# Memory Management Class\n",
    "class GPUMemoryManager:\n",
    "    \"\"\"Monitor and manage GPU memory during training\"\"\"\n",
    "    \n",
    "    def __init__(self, device_id=0):\n",
    "        self.device_id = device_id\n",
    "        self.has_gpu = torch.cuda.is_available()\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current memory usage information\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            ram = psutil.virtual_memory()\n",
    "            return {\n",
    "                'type': 'RAM',\n",
    "                'used_gb': ram.used / 1024**3,\n",
    "                'total_gb': ram.total / 1024**3,\n",
    "                'percent': ram.percent\n",
    "            }\n",
    "        \n",
    "        # GPU memory info\n",
    "        torch.cuda.synchronize(self.device_id)\n",
    "        allocated = torch.cuda.memory_allocated(self.device_id) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(self.device_id) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(self.device_id).total_memory / 1024**3\n",
    "        \n",
    "        return {\n",
    "            'type': 'GPU',\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'total_gb': total,\n",
    "            'percent': (allocated / total) * 100\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear GPU cache\"\"\"\n",
    "        if self.has_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def print_memory_summary(self, prefix=\"\"):\n",
    "        \"\"\"Print memory usage summary\"\"\"\n",
    "        info = self.get_memory_info()\n",
    "        if info['type'] == 'GPU':\n",
    "            print(f\"{prefix}GPU Memory: {info['allocated_gb']:.2f}GB/{info['total_gb']:.2f}GB ({info['percent']:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{prefix}RAM: {info['used_gb']:.2f}GB/{info['total_gb']:.2f}GB ({info['percent']:.1f}%)\")\n",
    "\n",
    "# Training Device Configuration\n",
    "has_gpu, gpu_count, current_gpu, flash_attention_hw_support = detect_training_hardware()\n",
    "flash_attention_available = check_flash_attention() and flash_attention_hw_support\n",
    "\n",
    "# Initialize memory manager\n",
    "memory_manager = GPUMemoryManager(current_gpu if current_gpu is not None else 0)\n",
    "memory_manager.print_memory_summary(\"üß† Initial \")\n",
    "\n",
    "# Device settings for training\n",
    "TRAINING_DEVICE = f\"cuda:{current_gpu}\" if has_gpu else \"cpu\"\n",
    "USE_FLASH_ATTENTION = has_gpu and gpu_count > 0  # Enable if GPU available\n",
    "USE_DISTRIBUTED = gpu_count > 1  # Multi-GPU training\n",
    "GPU_MEMORY_FRACTION = 0.95  # Use 95% of GPU memory for training\n",
    "\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"   Training device: {TRAINING_DEVICE}\")\n",
    "print(f\"   Flash Attention: {'Enabled' if USE_FLASH_ATTENTION else 'Disabled'}\")\n",
    "print(f\"   Distributed training: {'Enabled' if USE_DISTRIBUTED else 'Disabled'}\")\n",
    "print(f\"   GPU memory fraction: {GPU_MEMORY_FRACTION}\")\n",
    "\n",
    "# Set memory fraction if using GPU\n",
    "if has_gpu:\n",
    "    torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_FRACTION, device=current_gpu)\n",
    "    # Clear cache to start fresh\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Flash Attention imports (only if supported)\n",
    "flash_attention_available = False\n",
    "if USE_FLASH_ATTENTION:\n",
    "    try:\n",
    "        # Try importing flash attention\n",
    "        import flash_attn\n",
    "        from flash_attn import flash_attn_func, flash_attn_with_kvcache\n",
    "        flash_attention_available = True\n",
    "        print(f\"   ‚úÖ Flash Attention v{flash_attn.__version__} loaded successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Flash Attention not available: {e}\")\n",
    "        print(f\"   üí° Install with: pip install flash-attn --no-build-isolation\")\n",
    "        USE_FLASH_ATTENTION = False\n",
    "\n",
    "# Import nanotron components with robust error handling\n",
    "print(\"\\nüì¶ Importing nanotron components...\")\n",
    "\n",
    "try:\n",
    "    # Import nanotron configuration classes\n",
    "    from nanotron.config import (\n",
    "        Config, \n",
    "        LlamaConfig, \n",
    "        ParallelismArgs, \n",
    "        RandomInit,\n",
    "        DataArgs,\n",
    "        DatasetStageArgs,\n",
    "        LoggingArgs,\n",
    "        CheckpointsArgs,\n",
    "        GeneralArgs,\n",
    "        TokensArgs,\n",
    "        OptimizerArgs,\n",
    "        LRSchedulerArgs,\n",
    "        InfiniAttentionArgs\n",
    "    )\n",
    "    print(\"‚úÖ Configuration classes imported successfully\")\n",
    "    \n",
    "    # Import trainer\n",
    "    from nanotron.trainer import DistributedTrainer\n",
    "    print(\"‚úÖ DistributedTrainer imported successfully\")\n",
    "    \n",
    "    # Import dataloader utilities\n",
    "    from nanotron.dataloader import get_datasets, clm_process, get_train_dataloader\n",
    "    print(\"‚úÖ Dataloader utilities imported successfully\")\n",
    "    \n",
    "    # Import model classes\n",
    "    from nanotron.models.llama import LlamaForTraining\n",
    "    print(\"‚úÖ LLaMA model imported successfully\")\n",
    "    \n",
    "    # Import constants\n",
    "    from nanotron import constants\n",
    "    print(\"‚úÖ Constants imported successfully\")\n",
    "    \n",
    "    print(\"üéâ All nanotron components imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import nanotron components: {e}\")\n",
    "    print(\"\\nüõ†Ô∏è  Troubleshooting steps:\")\n",
    "    print(\"1. Install nanotron in development mode:\")\n",
    "    print(\"   cd /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini\")\n",
    "    print(\"   pip install -e .\")\n",
    "    print(\"2. Check if all dependencies are installed:\")\n",
    "    print(\"   pip install torch pyyaml numpy packaging safetensors dacite tqdm\")\n",
    "    print(\"3. Restart the kernel and try again\")\n",
    "    raise ImportError(\"Cannot proceed without nanotron - please install it first\")\n",
    "\n",
    "# Setup Infini attention constants - this must be done before importing models\n",
    "@dataclass\n",
    "class InfiniAttentionConfig:\n",
    "    segment_length: int = 64\n",
    "    turn_on_memory: bool = True\n",
    "    balance_factor_lr: float = 0.00015\n",
    "    balance_act_type: str = \"hard_sigmoid\"\n",
    "    balance_init_type: str = \"zeros\"\n",
    "    logging: bool = True\n",
    "    logging_interval: int = 1000\n",
    "    log_grad: bool = False\n",
    "    log_segment_acts: bool = False\n",
    "    balance_factor_weight_decay: Optional[float] = None\n",
    "    use_flash_attention: bool = USE_FLASH_ATTENTION  # Add Flash Attention flag\n",
    "\n",
    "@dataclass\n",
    "class ConfigWithInfini:\n",
    "    infini_attention: InfiniAttentionConfig = field(default_factory=InfiniAttentionConfig)\n",
    "\n",
    "# Set up the Infini attention configuration in constants\n",
    "constants.CONFIG = ConfigWithInfini()\n",
    "\n",
    "print(\"\\n‚úÖ All imports successful!\")\n",
    "print(f\"üî¨ Infini attention configured:\")\n",
    "print(f\"   - Segment length: {constants.CONFIG.infini_attention.segment_length}\")\n",
    "print(f\"   - Memory enabled: {constants.CONFIG.infini_attention.turn_on_memory}\")\n",
    "print(f\"   - Balance activation: {constants.CONFIG.infini_attention.balance_act_type}\")\n",
    "print(f\"   - Balance initialization: {constants.CONFIG.infini_attention.balance_init_type}\")\n",
    "print(f\"   - Flash Attention: {constants.CONFIG.infini_attention.use_flash_attention}\")\n",
    "print(f\"\\nüöÄ Ready for GPU training on {TRAINING_DEVICE}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e246d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nanotron in development mode\n",
    "print(\"\\nüîß Installing nanotron in development mode...\")\n",
    "nanotron_path = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini\"\n",
    "\n",
    "try:\n",
    "    # Change to nanotron directory and install in development mode\n",
    "    import os\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(nanotron_path)\n",
    "    \n",
    "    # Install nanotron in development mode\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ nanotron installed successfully in development mode\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning during nanotron installation: {result.stderr}\")\n",
    "        print(\"Continuing with path-based import...\")\n",
    "    \n",
    "    # Return to original directory\n",
    "    os.chdir(original_cwd)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not install nanotron in dev mode: {e}\")\n",
    "    print(\"Will try path-based import...\")\n",
    "\n",
    "# Add nanotron source to Python path as backup\n",
    "nanotron_src_path = os.path.join(nanotron_path, \"src\")\n",
    "if nanotron_src_path not in sys.path:\n",
    "    sys.path.insert(0, nanotron_src_path)\n",
    "    print(f\"üìÅ Added to Python path: {nanotron_src_path}\")\n",
    "\n",
    "# Verify nanotron can be imported\n",
    "try:\n",
    "    import nanotron\n",
    "    print(f\"‚úÖ nanotron imported successfully (version {nanotron.__version__})\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Still cannot import nanotron: {e}\")\n",
    "    print(\"\\nüõ†Ô∏è  Troubleshooting steps:\")\n",
    "    print(\"1. Check if you're in the correct directory\")\n",
    "    print(\"2. Try running: pip install -e . from the nanotron-infini directory\")\n",
    "    print(\"3. Check Python environment and dependencies\")\n",
    "    \n",
    "    # List what's actually in the nanotron source path\n",
    "    if os.path.exists(nanotron_src_path):\n",
    "        print(f\"\\nüìÇ Contents of {nanotron_src_path}:\")\n",
    "        for item in os.listdir(nanotron_src_path):\n",
    "            print(f\"   - {item}\")\n",
    "            \n",
    "        nanotron_module_path = os.path.join(nanotron_src_path, \"nanotron\")\n",
    "        if os.path.exists(nanotron_module_path):\n",
    "            print(f\"\\nüìÇ Contents of nanotron module:\")\n",
    "            for item in os.listdir(nanotron_module_path)[:10]:  # Show first 10 items\n",
    "                print(f\"   - {item}\")\n",
    "    \n",
    "    raise ImportError(\"Cannot proceed without nanotron module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ee7cb",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Define the training configuration for our 200M LLaMA model.\n",
    "\n",
    "## GPU Memory Management and Configuration\n",
    "\n",
    "Optimized memory management for training on different hardware configurations with Infini attention.\n",
    "\n",
    "**Features:**\n",
    "- üîç Automatic GPU detection and memory optimization\n",
    "- üìä Real-time memory monitoring during training\n",
    "- ‚ö° Flash Attention integration when supported\n",
    "- üîß Dynamic batch size adjustment based on GPU memory\n",
    "- üíæ Memory cleanup and cache management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for 200M parameters with Infini-Attention\n",
    "# Based on the proven fineweb_local_200m_infini_config.yaml\n",
    "# Optimized for GPU training with Flash Attention support\n",
    "\n",
    "model_config = LlamaConfig(\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_size=1024,         # Proven 200M config\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=4096,   # 4 * hidden_size for 200M model\n",
    "    max_position_embeddings=256,  # Start smaller, can be extended with Infini attention\n",
    "    num_attention_heads=8,    # Proven 200M config\n",
    "    num_hidden_layers=6,      # Proven 200M config\n",
    "    num_key_value_heads=8,    # Same as attention heads\n",
    "    pretraining_tp=1,\n",
    "    rms_norm_eps=1e-5,\n",
    "    rope_scaling=None,\n",
    "    rope_theta=10000.0,\n",
    "    tie_word_embeddings=False,  # Important: False for Infini attention\n",
    "    use_cache=True,\n",
    "    vocab_size=49152,         # Proven vocab size from the config\n",
    "    pad_token_id=None,\n",
    "    rope_interleaved=False,\n",
    "    is_using_mup=False,\n",
    "    # GPU and Flash Attention optimizations\n",
    "    use_flash_attention_2=USE_FLASH_ATTENTION,  # Enable Flash Attention 2 if supported\n",
    "    attention_dropout=0.0 if USE_FLASH_ATTENTION else 0.1,  # Disable dropout with Flash Attention\n",
    ")\n",
    "\n",
    "# GPU-optimized training configuration\n",
    "GPU_TRAINING_CONFIG = {\n",
    "    \"device\": TRAINING_DEVICE,\n",
    "    \"use_flash_attention\": USE_FLASH_ATTENTION,\n",
    "    \"use_distributed\": USE_DISTRIBUTED,\n",
    "    \"gpu_memory_fraction\": GPU_MEMORY_FRACTION,\n",
    "    \"mixed_precision\": has_gpu,  # Enable mixed precision on GPU\n",
    "    \"gradient_checkpointing\": has_gpu,  # Enable gradient checkpointing for memory efficiency\n",
    "    \"compile_model\": has_gpu and torch.__version__ >= \"2.0\",  # torch.compile for PyTorch 2.0+\n",
    "}\n",
    "\n",
    "print(f\"üîß GPU Training Configuration:\")\n",
    "for key, value in GPU_TRAINING_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# GPU Memory Management Functions\n",
    "class GPUMemoryManager:\n",
    "    \"\"\"Comprehensive GPU memory management for Infini attention training\"\"\"\n",
    "    \n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or TRAINING_DEVICE\n",
    "        self.memory_history = []\n",
    "        self.peak_memory = 0\n",
    "        \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current GPU memory usage information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"device\": \"cpu\", \"allocated\": 0, \"reserved\": 0, \"total\": 0}\n",
    "            \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "        \n",
    "        return {\n",
    "            \"device\": self.device,\n",
    "            \"allocated\": allocated,\n",
    "            \"reserved\": reserved,\n",
    "            \"total\": total,\n",
    "            \"free\": total - reserved,\n",
    "            \"usage_percent\": (allocated / total) * 100\n",
    "        }\n",
    "    \n",
    "    def monitor_memory(self, step_name=\"\", log=True):\n",
    "        \"\"\"Monitor and log memory usage\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        info = self.get_memory_info()\n",
    "        self.memory_history.append({\"step\": step_name, **info})\n",
    "        \n",
    "        if info[\"allocated\"] > self.peak_memory:\n",
    "            self.peak_memory = info[\"allocated\"]\n",
    "            \n",
    "        if log:\n",
    "            print(f\"üìä GPU Memory [{step_name}]:\")\n",
    "            print(f\"   Allocated: {info['allocated']:.2f} GB ({info['usage_percent']:.1f}%)\")\n",
    "            print(f\"   Reserved: {info['reserved']:.2f} GB\")\n",
    "            print(f\"   Free: {info['free']:.2f} GB\")\n",
    "            \n",
    "    def clear_cache(self, force=False):\n",
    "        \"\"\"Clear GPU cache and perform garbage collection\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        if force:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    def optimize_for_infini_attention(self):\n",
    "        \"\"\"Apply memory optimizations specific to Infini attention\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        print(\"üîß Applying Infini attention memory optimizations...\")\n",
    "        \n",
    "        # Set memory management environment variables\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        # Configure cuDNN for memory efficiency\n",
    "        if torch.backends.cudnn.is_available():\n",
    "            torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
    "            torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n",
    "            \n",
    "        # Set appropriate memory fraction\n",
    "        torch.cuda.set_per_process_memory_fraction(GPU_MEMORY_FRACTION)\n",
    "        \n",
    "        print(f\"   ‚úÖ Memory fraction set to {GPU_MEMORY_FRACTION}\")\n",
    "        print(f\"   ‚úÖ Memory allocation optimized for segmented processing\")\n",
    "        \n",
    "    def get_recommended_batch_size(self, model_size_mb=800):  # 200M model ‚âà 800MB\n",
    "        \"\"\"Calculate recommended batch size based on available GPU memory\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1\n",
    "            \n",
    "        info = self.get_memory_info()\n",
    "        available_memory_gb = info[\"free\"]\n",
    "        \n",
    "        # Conservative estimate: model + gradients + activations + buffer\n",
    "        memory_per_sample_mb = model_size_mb * 3  # Model, gradients, activations\n",
    "        memory_per_sample_gb = memory_per_sample_mb / 1024\n",
    "        \n",
    "        # Keep 2GB buffer for other operations\n",
    "        usable_memory_gb = max(0.5, available_memory_gb - 2.0)\n",
    "        \n",
    "        recommended_batch_size = max(1, int(usable_memory_gb / memory_per_sample_gb))\n",
    "        \n",
    "        print(f\"üí° Memory-based batch size recommendation:\")\n",
    "        print(f\"   Available GPU memory: {available_memory_gb:.2f} GB\")\n",
    "        print(f\"   Estimated memory per sample: {memory_per_sample_gb:.3f} GB\")\n",
    "        print(f\"   Recommended batch size: {recommended_batch_size}\")\n",
    "        \n",
    "        return recommended_batch_size\n",
    "        \n",
    "    def print_memory_summary(self):\n",
    "        \"\"\"Print comprehensive memory usage summary\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"üìä Memory Summary: CPU mode (no GPU available)\")\n",
    "            return\n",
    "            \n",
    "        current = self.get_memory_info()\n",
    "        \n",
    "        print(\"üìä GPU Memory Summary:\")\n",
    "        print(f\"   Device: {current['device']}\")\n",
    "        print(f\"   Total GPU Memory: {current['total']:.2f} GB\")\n",
    "        print(f\"   Currently Allocated: {current['allocated']:.2f} GB ({current['usage_percent']:.1f}%)\")\n",
    "        print(f\"   Currently Reserved: {current['reserved']:.2f} GB\")\n",
    "        print(f\"   Peak Usage (session): {self.peak_memory:.2f} GB\")\n",
    "        print(f\"   Free Memory: {current['free']:.2f} GB\")\n",
    "        \n",
    "        if len(self.memory_history) > 1:\n",
    "            print(f\"   Memory tracking points: {len(self.memory_history)}\")\n",
    "            \n",
    "# Initialize memory manager\n",
    "memory_manager = GPUMemoryManager()\n",
    "memory_manager.optimize_for_infini_attention()\n",
    "memory_manager.monitor_memory(\"Initial setup\", log=True)\n",
    "\n",
    "# Calculate approximate parameter count\n",
    "def estimate_parameters(config):\n",
    "    \"\"\"Rough estimate of model parameters\"\"\"\n",
    "    vocab_size = config.vocab_size\n",
    "    hidden_size = config.hidden_size\n",
    "    intermediate_size = config.intermediate_size\n",
    "    num_layers = config.num_hidden_layers\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embedding_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Per layer parameters\n",
    "    attention_params = 4 * hidden_size * hidden_size  # qkv + o projections\n",
    "    mlp_params = 3 * hidden_size * intermediate_size   # gate, up, down\n",
    "    layer_norm_params = 2 * hidden_size                # input and post attention layer norms\n",
    "    \n",
    "    # Infini attention adds balance factors per head\n",
    "    infini_params = num_layers * config.num_attention_heads  # balance factors\n",
    "    \n",
    "    per_layer_params = attention_params + mlp_params + layer_norm_params + (infini_params / num_layers)\n",
    "    total_layer_params = num_layers * per_layer_params\n",
    "    \n",
    "    # Final layer norm + lm_head (separate from embeddings when tie_word_embeddings=False)\n",
    "    final_params = hidden_size + vocab_size * hidden_size\n",
    "    \n",
    "    total_params = embedding_params + total_layer_params + final_params\n",
    "    return total_params\n",
    "\n",
    "estimated_params = estimate_parameters(model_config)\n",
    "print(f\"üìä Estimated parameters: {estimated_params:,} ({estimated_params/1e6:.1f}M)\")\n",
    "print(f\"üî¨ Infini-Attention features:\")\n",
    "print(f\"   - Segment length: {constants.CONFIG.infini_attention.segment_length}\")\n",
    "print(f\"   - Compressive memory: {constants.CONFIG.infini_attention.turn_on_memory}\")\n",
    "print(f\"   - Balance factor learning rate: {constants.CONFIG.infini_attention.balance_factor_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration based on proven fineweb_local_200m_infini_config.yaml\n",
    "config = Config(\n",
    "    general=GeneralArgs(\n",
    "        project=\"llama-200m-infini-training\",\n",
    "        run=\"llama-200m-infini-experiment\",\n",
    "        seed=42,\n",
    "        step=None,\n",
    "        consumed_train_samples=None,\n",
    "        ignore_sanity_checks=False  # Keep sanity checks for safety\n",
    "    ),\n",
    "    \n",
    "    checkpoints=CheckpointsArgs(\n",
    "        checkpoints_path=Path(\"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/checkpoints/llama_200m_infini\"),\n",
    "        checkpoint_interval=500,     # Save more frequently for testing\n",
    "        save_initial_state=False,\n",
    "        resume_checkpoint_path=None,\n",
    "        checkpoints_path_is_shared_file_system=False\n",
    "    ),\n",
    "    \n",
    "    parallelism=ParallelismArgs(\n",
    "        dp=1,                    # Data parallel size\n",
    "        pp=1,                    # Pipeline parallel size  \n",
    "        tp=1,                    # Tensor parallel size\n",
    "        pp_engine=\"1f1b\",        # Pipeline engine\n",
    "        tp_mode=\"ALL_REDUCE\",\n",
    "        tp_linear_async_communication=False,\n",
    "        expert_parallel_size=1\n",
    "    ),\n",
    "    \n",
    "    model=model_config,\n",
    "    \n",
    "    tokens=TokensArgs(\n",
    "        sequence_length=256,     # Start with proven config length\n",
    "        train_steps=5000,        # Reduced for initial testing\n",
    "        micro_batch_size=4,      # Proven batch size from config\n",
    "        batch_accumulation_per_replica=1,\n",
    "        limit_val_batches=0\n",
    "    ),\n",
    "    \n",
    "    optimizer=OptimizerArgs(\n",
    "        zero_stage=0,\n",
    "        weight_decay=0.1,        # Proven from config\n",
    "        clip_grad=1.0,\n",
    "        accumulate_grad_in_fp32=True,\n",
    "        adam_eps=1e-8,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.95,\n",
    "        torch_adam_is_fused=True,\n",
    "        learning_rate_scheduler=LRSchedulerArgs(\n",
    "            learning_rate=0.0000375,  # Proven learning rate from config\n",
    "            lr_warmup_steps=500,      # Reduced proportionally\n",
    "            lr_warmup_style=\"linear\",\n",
    "            lr_decay_steps=4500,      # Remaining steps after warmup\n",
    "            lr_decay_style=\"cosine\",\n",
    "            min_decay_lr=0.00000375   # 10x smaller than main LR\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    logging=LoggingArgs(\n",
    "        iteration_step_info_interval=50,  # More frequent logging for testing\n",
    "        log_level=\"info\",\n",
    "        log_level_replica=\"info\"\n",
    "    ),\n",
    "    \n",
    "    data_stages=[\n",
    "        DatasetStageArgs(\n",
    "            name=\"infini_training_stage\",\n",
    "            start_training_step=1,\n",
    "            data=DataArgs(\n",
    "                dataset={\n",
    "                    \"parquet\": {\n",
    "                        \"data_dir\": \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train\",\n",
    "                        \"data_files\": \"train_data.parquet\"\n",
    "                    }\n",
    "                },\n",
    "                seed=42,\n",
    "                num_loading_workers=0,  # Single process for testing\n",
    "                dataloader_type=\"single\"\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    # Add Infini attention configuration\n",
    "    infini_attention=InfiniAttentionArgs(\n",
    "        segment_length=constants.CONFIG.infini_attention.segment_length,\n",
    "        turn_on_memory=constants.CONFIG.infini_attention.turn_on_memory,\n",
    "        balance_factor_lr=constants.CONFIG.infini_attention.balance_factor_lr,\n",
    "        balance_act_type=constants.CONFIG.infini_attention.balance_act_type,\n",
    "        balance_init_type=constants.CONFIG.infini_attention.balance_init_type,\n",
    "        logging=constants.CONFIG.infini_attention.logging,\n",
    "        logging_interval=constants.CONFIG.infini_attention.logging_interval,\n",
    "        log_grad=constants.CONFIG.infini_attention.log_grad,\n",
    "        log_segment_acts=constants.CONFIG.infini_attention.log_segment_acts,\n",
    "        balance_factor_weight_decay=constants.CONFIG.infini_attention.balance_factor_weight_decay\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration created successfully!\")\n",
    "print(f\"üìà Global batch size: {config.tokens.micro_batch_size * config.tokens.batch_accumulation_per_replica * config.parallelism.dp}\")\n",
    "print(f\"üîÑ Total training steps: {config.tokens.train_steps}\")\n",
    "print(f\"üìè Sequence length: {config.tokens.sequence_length}\")\n",
    "print(f\"üìÅ Data directory: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train\")\n",
    "print(f\"üî¨ Infini attention segment length: {config.infini_attention.segment_length}\")\n",
    "print(f\"üß† Memory enabled: {config.infini_attention.turn_on_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd302ac1",
   "metadata": {},
   "source": [
    "## Data Loading Setup\n",
    "\n",
    "Prepare the dataset and dataloader for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the proven tokenizer from the config\n",
    "def setup_tokenizer():\n",
    "    \"\"\"Setup the proven tokenizer for training\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Use the proven tokenizer from the config\n",
    "        tokenizer_name = \"lvwerra/the-tokenizer-v1\"  # From fineweb_local_200m_infini_config.yaml\n",
    "        \n",
    "        print(f\"üî§ Loading tokenizer: {tokenizer_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "        # Verify vocab size matches our model config\n",
    "        if tokenizer.vocab_size != config.model.vocab_size:\n",
    "            print(f\"‚ö†Ô∏è  Tokenizer vocab size ({tokenizer.vocab_size}) doesn't match model vocab size ({config.model.vocab_size})\")\n",
    "            print(f\"   This is expected - the model vocab size includes padding for efficiency\")\n",
    "            \n",
    "        # Set special tokens to match our model config\n",
    "        if hasattr(tokenizer, 'pad_token_id'):\n",
    "            tokenizer.pad_token_id = config.model.pad_token_id or 0\n",
    "        if hasattr(tokenizer, 'bos_token_id'):\n",
    "            tokenizer.bos_token_id = config.model.bos_token_id\n",
    "        if hasattr(tokenizer, 'eos_token_id'):\n",
    "            tokenizer.eos_token_id = config.model.eos_token_id\n",
    "        \n",
    "        print(f\"‚úÖ Using proven AutoTokenizer: {tokenizer.__class__.__name__}\")\n",
    "        return tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load proven tokenizer ({e}), using GPT-2 fallback\")\n",
    "        \n",
    "        # Fallback to GPT-2 tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "            # Add padding token if missing\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "            # Set special tokens\n",
    "            tokenizer.pad_token_id = 0\n",
    "            tokenizer.bos_token_id = config.model.bos_token_id\n",
    "            tokenizer.eos_token_id = config.model.eos_token_id\n",
    "            \n",
    "            print(f\"‚úÖ Using GPT-2 fallback tokenizer\")\n",
    "            return tokenizer\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Could not load any tokenizer ({e2}), using simple tokenizer\")\n",
    "            \n",
    "            # Simple tokenizer fallback\n",
    "            class SimpleTokenizer:\n",
    "                \"\"\"A simple tokenizer for demonstration purposes\"\"\"\n",
    "                def __init__(self, vocab_size=49152):\n",
    "                    self.vocab_size = vocab_size\n",
    "                    self.pad_token_id = 0\n",
    "                    self.bos_token_id = 1\n",
    "                    self.eos_token_id = 2\n",
    "                    \n",
    "                def encode(self, text, add_special_tokens=True):\n",
    "                    \"\"\"Simple encoding that converts text to token IDs\"\"\"\n",
    "                    import re\n",
    "                    # Simple preprocessing: lowercase, split on whitespace and punctuation\n",
    "                    words = re.findall(r'\\w+|[^\\w\\s]', str(text).lower())\n",
    "                    # Convert to token IDs (hash-based for simplicity)\n",
    "                    tokens = [hash(word) % (self.vocab_size - 10) + 10 for word in words]\n",
    "                    if add_special_tokens:\n",
    "                        tokens = [self.bos_token_id] + tokens + [self.eos_token_id]\n",
    "                    return tokens\n",
    "                    \n",
    "                def batch_encode_plus(self, texts, return_attention_mask=False, return_token_type_ids=False):\n",
    "                    \"\"\"Simple batch encoding\"\"\"\n",
    "                    all_tokens = []\n",
    "                    for text in texts:\n",
    "                        tokens = self.encode(text, add_special_tokens=True)\n",
    "                        all_tokens.append(tokens)\n",
    "                    return {\"input_ids\": all_tokens}\n",
    "            \n",
    "            return SimpleTokenizer(vocab_size=config.model.vocab_size)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = setup_tokenizer()\n",
    "print(f\"‚úÖ Tokenizer initialized: {tokenizer.__class__.__name__}\")\n",
    "print(f\"   üìä Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"   üî§ Special tokens: BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={getattr(tokenizer, 'pad_token_id', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(config, tokenizer):\n",
    "    \"\"\"Load data from our preprocessed parquet files for Infini attention training\"\"\"\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset, Features, Value\n",
    "    \n",
    "    try:\n",
    "        # Load the preprocessed training data\n",
    "        train_data_path = \"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train/train_data.parquet\"\n",
    "        \n",
    "        print(f\"üìö Loading preprocessed training data from: {train_data_path}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(train_data_path):\n",
    "            raise FileNotFoundError(f\"Training data not found at {train_data_path}\")\n",
    "        \n",
    "        # Load the parquet file\n",
    "        df = pd.read_parquet(train_data_path)\n",
    "        print(f\"‚úÖ Loaded dataframe with shape: {df.shape}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        if len(df) > 0:\n",
    "            print(f\"üëÄ Sample data:\")\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':  # Text columns\n",
    "                    sample_text = str(df[col].iloc[0])[:100] + \"...\" if len(str(df[col].iloc[0])) > 100 else str(df[col].iloc[0])\n",
    "                    print(f\"   {col}: {sample_text}\")\n",
    "                else:\n",
    "                    print(f\"   {col}: {df[col].iloc[0]}\")\n",
    "        \n",
    "        # Determine text column - look for 'text' first, then any string column\n",
    "        text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        if 'text' in df.columns:\n",
    "            text_column = 'text'\n",
    "        elif len(text_columns) > 0:\n",
    "            text_column = text_columns[0]\n",
    "            print(f\"‚ö†Ô∏è  No 'text' column found, using '{text_column}' instead\")\n",
    "        else:\n",
    "            raise ValueError(\"No text column found in the dataset\")\n",
    "        \n",
    "        # Extract texts\n",
    "        texts = df[text_column].dropna().astype(str).tolist()\n",
    "        print(f\"üìù Found {len(texts)} text samples\")\n",
    "        \n",
    "        # Take a reasonable subset for training (adjust based on your needs)\n",
    "        max_samples = 50000  # Start with a manageable size\n",
    "        if len(texts) > max_samples:\n",
    "            texts = texts[:max_samples]\n",
    "            print(f\"üìä Using subset of {len(texts)} samples for training\")\n",
    "        \n",
    "        # Filter out very short texts\n",
    "        min_text_length = 50  # Minimum characters\n",
    "        texts = [text for text in texts if len(text.strip()) >= min_text_length]\n",
    "        print(f\"üìù After filtering short texts: {len(texts)} samples\")\n",
    "        \n",
    "        if len(texts) == 0:\n",
    "            raise ValueError(\"No valid text samples found after filtering\")\n",
    "        \n",
    "        # Create HuggingFace dataset\n",
    "        dataset_dict = {\"text\": texts}\n",
    "        train_dataset = Dataset.from_dict(dataset_dict)\n",
    "        \n",
    "        print(f\"‚úÖ Created HuggingFace dataset with {len(train_dataset)} samples\")\n",
    "        \n",
    "        # Process for causal language modeling with the proven tokenizer\n",
    "        print(\"üîÑ Processing dataset for CLM with Infini attention...\")\n",
    "        processed_dataset = clm_process(\n",
    "            raw_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            text_column_name=\"text\",\n",
    "            dataset_processing_num_proc_per_process=1,\n",
    "            dataset_overwrite_cache=True,\n",
    "            sequence_length=config.tokens.sequence_length\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset processed: {len(processed_dataset)} sequences\")\n",
    "        print(f\"üî¨ Ready for Infini attention training with segment length: {config.infini_attention.segment_length}\")\n",
    "        return processed_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading preprocessed data: {e}\")\n",
    "        print(\"üîÑ Creating dummy dataset for demonstration...\")\n",
    "        \n",
    "        # Create dummy dataset as fallback\n",
    "        import numpy as np\n",
    "        from datasets import Features, Sequence, Value\n",
    "        \n",
    "        # Generate dummy sequences for Infini attention testing\n",
    "        num_samples = 1000\n",
    "        dummy_data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Create random sequence of token IDs\n",
    "            seq_length = config.tokens.sequence_length + 1  # +1 for CLM processing\n",
    "            tokens = np.random.randint(10, min(config.model.vocab_size-10, tokenizer.vocab_size), size=seq_length)\n",
    "            # Ensure BOS and EOS tokens\n",
    "            tokens[0] = tokenizer.bos_token_id\n",
    "            tokens[-1] = tokenizer.eos_token_id\n",
    "            dummy_data.append({\"input_ids\": tokens.tolist()})\n",
    "        \n",
    "        dummy_dataset = Dataset.from_list(\n",
    "            dummy_data,\n",
    "            features=Features({\"input_ids\": Sequence(feature=Value(dtype=\"int64\"), length=config.tokens.sequence_length + 1)})\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dummy dataset created: {len(dummy_dataset)} sequences\")\n",
    "        print(f\"üî¨ Ready for Infini attention testing\")\n",
    "        return dummy_dataset\n",
    "\n",
    "# Load the preprocessed training dataset\n",
    "train_dataset = load_preprocessed_data(config, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc207e",
   "metadata": {},
   "source": [
    "## Model Initialization and Training\n",
    "\n",
    "Initialize the trainer and start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6925429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config to file for trainer\n",
    "config_path = Path(\"/Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/configs/llama_200m_infini_config.yaml\")\n",
    "config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert config to dict and save with proper YAML formatting\n",
    "try:\n",
    "    config_dict = config.as_dict()\n",
    "except AttributeError:\n",
    "    # Fallback manual serialization if as_dict doesn't work\n",
    "    config_dict = {\n",
    "        'general': asdict(config.general),\n",
    "        'checkpoints': asdict(config.checkpoints),\n",
    "        'parallelism': asdict(config.parallelism),\n",
    "        'model': asdict(config.model),\n",
    "        'tokens': asdict(config.tokens),\n",
    "        'optimizer': asdict(config.optimizer),\n",
    "        'logging': asdict(config.logging),\n",
    "        'data_stages': [asdict(stage) for stage in config.data_stages],\n",
    "        'infini_attention': asdict(config.infini_attention)\n",
    "    }\n",
    "\n",
    "# Convert Path objects to strings for YAML serialization\n",
    "def convert_paths_to_strings(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_paths_to_strings(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_paths_to_strings(item) for item in obj]\n",
    "    elif isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "config_dict = convert_paths_to_strings(config_dict)\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Infini attention config saved to: {config_path}\")\n",
    "print(\"üî¨ Configuration includes:\")\n",
    "print(f\"   - Segment length: {config.infini_attention.segment_length}\")\n",
    "print(f\"   - Memory enabled: {config.infini_attention.turn_on_memory}\")\n",
    "print(f\"   - Balance factor LR: {config.infini_attention.balance_factor_lr}\")\n",
    "print(f\"   - Balance activation: {config.infini_attention.balance_act_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distributed environment for Infini attention training\n",
    "import torch.distributed as dist\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"Initialize distributed training environment for Infini attention\"\"\"\n",
    "    try:\n",
    "        if not dist.is_initialized():\n",
    "            # For single GPU training\n",
    "            os.environ.setdefault('RANK', '0')\n",
    "            os.environ.setdefault('WORLD_SIZE', '1') \n",
    "            os.environ.setdefault('MASTER_ADDR', 'localhost')\n",
    "            os.environ.setdefault('MASTER_PORT', '12355')\n",
    "            \n",
    "            # Set CUDA device if available\n",
    "            if torch.cuda.is_available():\n",
    "                backend = 'nccl'\n",
    "                torch.cuda.set_device(0)\n",
    "                print(f\"üî• Using GPU for Infini attention training\")\n",
    "            else:\n",
    "                backend = 'gloo'\n",
    "                print(\"‚ö†Ô∏è  CUDA not available, using CPU (not recommended for Infini attention)\")\n",
    "            \n",
    "            dist.init_process_group(\n",
    "                backend=backend,\n",
    "                init_method='env://',\n",
    "                world_size=1,\n",
    "                rank=0\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Distributed environment initialized ({backend} backend)\")\n",
    "        else:\n",
    "            print(\"‚úÖ Distributed environment already initialized\")\n",
    "            \n",
    "        # Set up for Infini attention training\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()  # Clear any existing cache\n",
    "            print(f\"üî• GPU memory cleared for Infini attention training\")\n",
    "            print(f\"   Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing distributed environment: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Continuing without distributed training (may affect performance)\")\n",
    "        return False\n",
    "\n",
    "# Initialize distributed environment\n",
    "dist_success = init_distributed()\n",
    "\n",
    "if dist_success:\n",
    "    print(\"üî¨ Infini attention training environment ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Training may proceed but with limited functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_dict(train_dataset, config):\n",
    "    \"\"\"Create dataloader dictionary for Infini attention trainer\"\"\"\n",
    "    from nanotron.parallel import ParallelContext\n",
    "    \n",
    "    try:\n",
    "        # Create proper parallel context for Infini attention\n",
    "        parallel_context = ParallelContext(\n",
    "            tensor_parallel_size=config.parallelism.tp,\n",
    "            pipeline_parallel_size=config.parallelism.pp,\n",
    "            data_parallel_size=config.parallelism.dp,\n",
    "            expert_parallel_size=config.parallelism.expert_parallel_size,\n",
    "        )\n",
    "        print(\"‚úÖ Parallel context created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Using mock parallel context for single GPU: {e}\")\n",
    "        \n",
    "        # Mock parallel context for single GPU training\n",
    "        class MockParallelContext:\n",
    "            def __init__(self):\n",
    "                if dist.is_initialized():\n",
    "                    self.world_pg = dist.group.WORLD\n",
    "                    self.dp_pg = dist.group.WORLD  \n",
    "                    self.tp_pg = dist.group.WORLD\n",
    "                    self.pp_pg = dist.group.WORLD\n",
    "                    self.expert_pg = dist.group.WORLD\n",
    "                else:\n",
    "                    self.world_pg = None\n",
    "                    self.dp_pg = None\n",
    "                    self.tp_pg = None\n",
    "                    self.pp_pg = None\n",
    "                    self.expert_pg = None\n",
    "                    \n",
    "        parallel_context = MockParallelContext()\n",
    "    \n",
    "    # Create dataloader for Infini attention training\n",
    "    try:\n",
    "        dataloader = get_train_dataloader(\n",
    "            train_dataset=train_dataset,\n",
    "            sequence_length=config.tokens.sequence_length,\n",
    "            parallel_context=parallel_context,\n",
    "            input_pp_rank=0,\n",
    "            output_pp_rank=0,\n",
    "            micro_batch_size=config.tokens.micro_batch_size,\n",
    "            consumed_train_samples=0,\n",
    "            dataloader_num_workers=0,  # Keep simple for testing\n",
    "            seed_worker=config.general.seed,\n",
    "            dataloader_drop_last=True,\n",
    "            dataloader_pin_memory=False,  # Disable for stability\n",
    "            use_loop_to_round_batch_size=False,\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Infini attention dataloader created successfully\")\n",
    "        print(f\"üîÑ Batch size: {config.tokens.micro_batch_size}\")\n",
    "        print(f\"üìè Sequence length: {config.tokens.sequence_length}\")\n",
    "        print(f\"üî¨ Segment length: {config.infini_attention.segment_length}\")\n",
    "        \n",
    "        # Calculate how many segments per sequence\n",
    "        segments_per_seq = config.tokens.sequence_length // config.infini_attention.segment_length\n",
    "        if config.tokens.sequence_length % config.infini_attention.segment_length != 0:\n",
    "            segments_per_seq += 1\n",
    "        print(f\"üß© Segments per sequence: {segments_per_seq}\")\n",
    "        \n",
    "        return {\"infini_training_stage\": dataloader}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating Infini attention dataloader: {e}\")\n",
    "        print(\"üîÑ Creating simple dataloader for testing...\")\n",
    "        \n",
    "        # Create a simple dummy dataloader for testing\n",
    "        from torch.utils.data import DataLoader, Dataset\n",
    "        \n",
    "        class InfiniDummyDataset(Dataset):\n",
    "            def __init__(self, size, seq_len, vocab_size):\n",
    "                self.size = size\n",
    "                self.seq_len = seq_len\n",
    "                self.vocab_size = vocab_size\n",
    "                \n",
    "            def __len__(self):\n",
    "                return self.size\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                # Create data suitable for Infini attention\n",
    "                input_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
    "                input_mask = torch.ones(self.seq_len, dtype=torch.bool)\n",
    "                label_ids = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
    "                label_mask = torch.ones(self.seq_len, dtype=torch.bool)\n",
    "                \n",
    "                return {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"input_mask\": input_mask,\n",
    "                    \"label_ids\": label_ids,\n",
    "                    \"label_mask\": label_mask,\n",
    "                }\n",
    "        \n",
    "        dummy_dataset = InfiniDummyDataset(\n",
    "            size=1000, \n",
    "            seq_len=config.tokens.sequence_length, \n",
    "            vocab_size=min(config.model.vocab_size, 32000)  # Cap for testing\n",
    "        )\n",
    "        \n",
    "        dummy_dataloader = DataLoader(\n",
    "            dummy_dataset,\n",
    "            batch_size=config.tokens.micro_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Dummy Infini attention dataloader created\")\n",
    "        return {\"infini_training_stage\": dummy_dataloader}\n",
    "\n",
    "# Create dataloader for Infini attention training\n",
    "dataloader_dict = create_dataloader_dict(train_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Infini attention trainer\n",
    "print(\"üöÄ Initializing Infini attention trainer...\")\n",
    "\n",
    "try:\n",
    "    # Ensure the config path is properly set for the trainer\n",
    "    trainer = DistributedTrainer(\n",
    "        config_or_config_file=config,\n",
    "        model_class=LlamaForTraining  # This will use Infini attention with our constants setup\n",
    "    )\n",
    "    print(\"‚úÖ Infini attention trainer initialized successfully!\")\n",
    "    \n",
    "    # Print model information\n",
    "    print(f\"üìä Model: {trainer.unwrapped_model.__class__.__name__}\")\n",
    "    \n",
    "    # Count actual parameters\n",
    "    total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"üìà Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "    print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
    "    print(f\"üíæ Model size (estimated): {total_params * 4 / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Check for Infini attention specific components\n",
    "    infini_attention_layers = 0\n",
    "    balance_factors_count = 0\n",
    "    \n",
    "    for name, module in trainer.unwrapped_model.named_modules():\n",
    "        if hasattr(module, 'segment_length'):\n",
    "            infini_attention_layers += 1\n",
    "        if hasattr(module, 'balance_factors'):\n",
    "            balance_factors_count += 1\n",
    "    \n",
    "    print(f\"üî¨ Infini attention components:\")\n",
    "    print(f\"   - Layers with segmentation: {infini_attention_layers}\")\n",
    "    print(f\"   - Balance factors: {balance_factors_count}\")\n",
    "    print(f\"   - Segment length: {constants.CONFIG.infini_attention.segment_length}\")\n",
    "    print(f\"   - Memory enabled: {constants.CONFIG.infini_attention.turn_on_memory}\")\n",
    "    print(f\"   - Balance activation: {constants.CONFIG.infini_attention.balance_act_type}\")\n",
    "    \n",
    "    # Check GPU memory usage if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"üî• GPU Memory:\")\n",
    "        print(f\"   - Allocated: {allocated:.1f} MB\")\n",
    "        print(f\"   - Reserved: {reserved:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Infini attention trainer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    trainer = None\n",
    "    \n",
    "    print(\"‚ö†Ô∏è  Troubleshooting tips:\")\n",
    "    print(\"1. Ensure all Infini attention constants are properly set\")\n",
    "    print(\"2. Check that the configuration is valid\")\n",
    "    print(\"3. Verify GPU memory is sufficient\")\n",
    "    print(\"4. Try reducing batch size if out of memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0049f7",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Add missing import\n",
    "\n",
    "if trainer is not None:\n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Starting Infini attention training...\")\n",
    "    print(f\"üî¨ Training with {config.infini_attention.segment_length}-token segments\")\n",
    "    print(f\"üß† Memory state: {'enabled' if config.infini_attention.turn_on_memory else 'disabled'}\")\n",
    "    \n",
    "    try:\n",
    "        # Pre-training setup\n",
    "        start_time = time.time()\n",
    "        initial_step = getattr(trainer, 'iteration_step', 0)\n",
    "        \n",
    "        print(f\"üìÖ Training started at step {initial_step}\")\n",
    "        print(f\"üéØ Target steps: {config.tokens.train_steps}\")\n",
    "        \n",
    "        # Start Infini attention training\n",
    "        trainer.train(dataloader_or_dls=dataloader_dict)\n",
    "        \n",
    "        # Calculate training time\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        print(\"üéâ Infini attention training completed successfully!\")\n",
    "        print(f\"‚è±Ô∏è  Training time: {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")\n",
    "        \n",
    "        # Final statistics\n",
    "        final_step = getattr(trainer, 'iteration_step', 0)\n",
    "        steps_completed = final_step - initial_step\n",
    "        \n",
    "        print(f\"üìà Training summary:\")\n",
    "        print(f\"   - Steps completed: {steps_completed}\")\n",
    "        print(f\"   - Average time per step: {training_time/max(steps_completed, 1):.2f} seconds\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   - Peak GPU memory: {torch.cuda.max_memory_allocated() / 1024**2:.1f} MB\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"‚ö†Ô∏è  Training interrupted by user\")\n",
    "        \n",
    "        # Try to save checkpoint if possible\n",
    "        try:\n",
    "            if hasattr(trainer, 'save_checkpoint'):\n",
    "                checkpoint_path = trainer.save_checkpoint()\n",
    "                print(f\"üíæ Emergency checkpoint saved to: {checkpoint_path}\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"‚ùå Could not save emergency checkpoint: {save_error}\")\n",
    "            \n",
    "    except torch.cuda.OutOfMemoryError as oom_error:\n",
    "        print(f\"‚ùå GPU out of memory during Infini attention training: {oom_error}\")\n",
    "        print(\"üí° Suggestions to fix:\")\n",
    "        print(\"   1. Reduce micro_batch_size in config\")\n",
    "        print(\"   2. Reduce sequence_length\")\n",
    "        print(\"   3. Reduce segment_length for Infini attention\")\n",
    "        print(\"   4. Enable gradient checkpointing\")\n",
    "        print(\"   5. Use mixed precision training\")\n",
    "        \n",
    "        # Try to save what we can\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            if hasattr(trainer, 'save_checkpoint'):\n",
    "                checkpoint_path = trainer.save_checkpoint()\n",
    "                print(f\"üíæ Emergency checkpoint saved to: {checkpoint_path}\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"‚ùå Could not save emergency checkpoint: {save_error}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during Infini attention training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Try to save checkpoint if possible\n",
    "        try:\n",
    "            if hasattr(trainer, 'save_checkpoint'):\n",
    "                checkpoint_path = trainer.save_checkpoint()\n",
    "                print(f\"üíæ Emergency checkpoint saved to: {checkpoint_path}\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"‚ùå Could not save emergency checkpoint: {save_error}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - Infini attention trainer initialization failed\")\n",
    "    print(\"üí° Please check the error messages above and:\")\n",
    "    print(\"   1. Verify the configuration is correct\")\n",
    "    print(\"   2. Ensure sufficient GPU memory\")\n",
    "    print(\"   3. Check that all dependencies are installed\")\n",
    "    print(\"   4. Try running the data preprocessing notebook first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24ad3b",
   "metadata": {},
   "source": [
    "## Training Monitoring and Results\n",
    "\n",
    "Monitor training progress and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infini attention training monitoring and results\n",
    "if trainer is not None:\n",
    "    print(\"üìà Infini Attention Training Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic training stats\n",
    "    try:\n",
    "        current_step = getattr(trainer, 'iteration_step', 0)\n",
    "        consumed_samples = getattr(trainer, 'consumed_train_samples', 0)\n",
    "        \n",
    "        print(f\"‚úÖ Training steps completed: {current_step - 1}\")\n",
    "        print(f\"üìà Consumed samples: {consumed_samples:,}\")\n",
    "        \n",
    "        # Calculate effective context\n",
    "        sequence_length = config.tokens.sequence_length\n",
    "        segment_length = config.infini_attention.segment_length\n",
    "        segments_per_sequence = sequence_length // segment_length\n",
    "        \n",
    "        print(f\"üìè Sequence length: {sequence_length}\")\n",
    "        print(f\"üî¨ Segment length: {segment_length}\")\n",
    "        print(f\"üß© Segments per sequence: {segments_per_sequence}\")\n",
    "        print(f\"üß† Memory-augmented context: Infinite (via compressive memory)\")\n",
    "        \n",
    "        # Batch information\n",
    "        global_batch_size = config.tokens.micro_batch_size * config.tokens.batch_accumulation_per_replica * config.parallelism.dp\n",
    "        print(f\"üî¢ Global batch size: {global_batch_size}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not retrieve training stats: {e}\")\n",
    "    \n",
    "    # Check checkpoints\n",
    "    checkpoints_dir = config.checkpoints.checkpoints_path\n",
    "    if checkpoints_dir.exists():\n",
    "        checkpoint_folders = list(checkpoints_dir.glob(\"*/\"))\n",
    "        if checkpoint_folders:\n",
    "            print(f\"üíæ Infini attention checkpoints saved: {len(checkpoint_folders)}\")\n",
    "            for cp in sorted(checkpoint_folders)[-3:]:  # Show last 3 checkpoints\n",
    "                size_mb = sum(f.stat().st_size for f in cp.rglob('*') if f.is_file()) / 1024**2\n",
    "                print(f\"   üìÅ {cp.name} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(\"‚ùå No checkpoints found\")\n",
    "    else:\n",
    "        print(f\"‚ùå Checkpoint directory does not exist: {checkpoints_dir}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Memory Usage:\")\n",
    "        print(f\"   - Current allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "        print(f\"   - Current reserved: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")\n",
    "        print(f\"   - Peak allocated: {torch.cuda.max_memory_allocated() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        # Calculate memory efficiency\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "        memory_usage_pct = (torch.cuda.max_memory_allocated() / (total_memory * 1024**2)) * 100\n",
    "        print(f\"   - Peak usage: {memory_usage_pct:.1f}% of total GPU memory\")\n",
    "        \n",
    "    # Infini attention specific metrics\n",
    "    print(f\"üî¨ Infini Attention Configuration:\")\n",
    "    print(f\"   - Segment processing: {config.infini_attention.segment_length} tokens per segment\")\n",
    "    print(f\"   - Compressive memory: {'Enabled' if config.infini_attention.turn_on_memory else 'Disabled'}\")\n",
    "    print(f\"   - Balance factor LR: {config.infini_attention.balance_factor_lr}\")\n",
    "    print(f\"   - Balance activation: {config.infini_attention.balance_act_type}\")\n",
    "    print(f\"   - Balance initialization: {config.infini_attention.balance_init_type}\")\n",
    "    \n",
    "    # Model architecture summary\n",
    "    if hasattr(trainer, 'unwrapped_model'):\n",
    "        total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "        print(f\"üè† Model Architecture:\")\n",
    "        print(f\"   - Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "        print(f\"   - Hidden size: {config.model.hidden_size}\")\n",
    "        print(f\"   - Attention heads: {config.model.num_attention_heads}\")\n",
    "        print(f\"   - Layers: {config.model.num_hidden_layers}\")\n",
    "        print(f\"   - Vocab size: {config.model.vocab_size}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No training results available - trainer initialization failed\")\n",
    "    print(\"üí° Check the error messages above for troubleshooting steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infini attention model inference test\n",
    "if trainer is not None:\n",
    "    try:\n",
    "        print(\"üß™ Testing Infini attention model inference...\")\n",
    "        \n",
    "        # Put model in eval mode\n",
    "        trainer.model.eval()\n",
    "        \n",
    "        # Create test input suitable for Infini attention\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Test with different sequence lengths to verify Infini attention\n",
    "        test_lengths = [32, 128, config.tokens.sequence_length]\n",
    "        \n",
    "        for seq_len in test_lengths:\n",
    "            print(f\"\\nüìè Testing sequence length: {seq_len}\")\n",
    "            \n",
    "            # Create test input\n",
    "            test_input = torch.randint(\n",
    "                10, min(config.model.vocab_size - 10, 1000),  # Use smaller vocab for testing\n",
    "                (1, seq_len),  # batch_size=1\n",
    "                device=device\n",
    "            )\n",
    "            test_mask = torch.ones_like(test_input, dtype=torch.bool)\n",
    "            \n",
    "            # Ensure proper token boundaries\n",
    "            test_input[0, 0] = tokenizer.bos_token_id  # Start with BOS\n",
    "            test_input[0, -1] = tokenizer.eos_token_id  # End with EOS\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Test forward pass with Infini attention\n",
    "                start_time = time.time()\n",
    "                \n",
    "                outputs = trainer.unwrapped_model.model.forward(\n",
    "                    input_ids=test_input,\n",
    "                    input_mask=test_mask\n",
    "                )\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Extract logits from outputs\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = outputs.get('hidden_states', outputs.get('logits', None))\n",
    "                elif hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                \n",
    "                if logits is not None:\n",
    "                    print(f\"   ‚úÖ Inference successful!\")\n",
    "                    print(f\"   üìâ Input shape: {test_input.shape}\")\n",
    "                    print(f\"   üìâ Output shape: {logits.shape}\")\n",
    "                    print(f\"   ‚è±Ô∏è  Inference time: {inference_time*1000:.2f} ms\")\n",
    "                    print(f\"   üéØ Max output value: {logits.max().item():.3f}\")\n",
    "                    print(f\"   üéØ Min output value: {logits.min().item():.3f}\")\n",
    "                    \n",
    "                    # Calculate segments processed\n",
    "                    segments = seq_len // config.infini_attention.segment_length\n",
    "                    if seq_len % config.infini_attention.segment_length != 0:\n",
    "                        segments += 1\n",
    "                    print(f\"   üß© Segments processed: {segments}\")\n",
    "                    \n",
    "                    # Memory usage for this inference\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_used = torch.cuda.memory_allocated() / 1024**2\n",
    "                        print(f\"   üî• GPU memory: {memory_used:.1f} MB\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Could not extract logits from output\")\n",
    "            \n",
    "            # Clear cache between tests\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Test segmented processing specifically\n",
    "        print(f\"\\nüî¨ Testing Infini attention segmentation:\")\n",
    "        \n",
    "        # Create a longer sequence to test segmentation\n",
    "        long_seq_len = config.infini_attention.segment_length * 3 + 10  # Multiple segments\n",
    "        print(f\"   Testing with {long_seq_len} tokens ({long_seq_len // config.infini_attention.segment_length + 1} segments)\")\n",
    "        \n",
    "        long_test_input = torch.randint(\n",
    "            10, min(config.model.vocab_size - 10, 1000),\n",
    "            (1, long_seq_len),\n",
    "            device=device\n",
    "        )\n",
    "        long_test_mask = torch.ones_like(long_test_input, dtype=torch.bool)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            long_outputs = trainer.unwrapped_model.model.forward(\n",
    "                input_ids=long_test_input,\n",
    "                input_mask=long_test_mask\n",
    "            )\n",
    "            segmented_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   ‚úÖ Segmented processing successful!\")\n",
    "            print(f\"   ‚è±Ô∏è  Processing time: {segmented_time*1000:.2f} ms\")\n",
    "            print(f\"   üß† Memory state: {'Active' if config.infini_attention.turn_on_memory else 'Inactive'}\")\n",
    "        \n",
    "        print(f\"\\nüéâ All Infini attention inference tests passed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Infini attention inference test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(f\"\\nüí° Troubleshooting:\")\n",
    "        print(\"   1. Check if model is properly initialized with Infini attention\")\n",
    "        print(\"   2. Verify input dimensions are correct\")\n",
    "        print(\"   3. Ensure sufficient GPU memory\")\n",
    "        print(\"   4. Try with smaller sequence lengths\")\n",
    "else:\n",
    "    print(\"‚ùå No inference test possible - trainer not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfb2c7",
   "metadata": {},
   "source": [
    "## Cleanup and Next Steps\n",
    "\n",
    "Clean up resources and provide guidance for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56764b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Infini attention training resources\n",
    "print(\"üßπ Cleaning up Infini attention training session...\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"‚úÖ CUDA cache cleared\")\n",
    "    \n",
    "    # Final memory report\n",
    "    final_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    print(f\"   Final GPU memory usage: {final_memory:.1f} MB\")\n",
    "\n",
    "# Reset model to free memory\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    # Try to save final state if training was successful\n",
    "    try:\n",
    "        if hasattr(trainer, 'save_checkpoint'):\n",
    "            final_checkpoint = trainer.save_checkpoint()\n",
    "            print(f\"üíæ Final checkpoint saved: {final_checkpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save final checkpoint: {e}\")\n",
    "    \n",
    "    del trainer\n",
    "    print(\"‚úÖ Infini attention trainer object deleted\")\n",
    "\n",
    "# Reset constants to clean state\n",
    "try:\n",
    "    if hasattr(constants, 'CONFIG'):\n",
    "        # Don't delete, just note the state\n",
    "        print(\"‚úÖ Infini attention constants preserved for future use\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"üéâ Infini attention training session completed!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù INFINI ATTENTION TRAINING SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"üè† Model: LLaMA 200M with Infini-Attention\")\n",
    "print(\"üìö Dataset: Preprocessed parquet data\")\n",
    "print(f\"üìÅ Data source: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train/train_data.parquet\")\n",
    "print(f\"üî¨ Segment Length: {config.infini_attention.segment_length} tokens\")\n",
    "print(f\"üß† Compressive Memory: {'Enabled' if config.infini_attention.turn_on_memory else 'Disabled'}\")\n",
    "print(f\"‚öñÔ∏è  Balance Activation: {config.infini_attention.balance_act_type}\")\n",
    "print(f\"üìè Sequence Length: {config.tokens.sequence_length}\")\n",
    "print(f\"üî¢ Micro Batch Size: {config.tokens.micro_batch_size}\")\n",
    "print(f\"üîÑ Training Steps: {config.tokens.train_steps}\")\n",
    "print(f\"üìà Learning Rate: {config.optimizer.learning_rate_scheduler.learning_rate}\")\n",
    "print(f\"üíæ Checkpoints: {config.checkpoints.checkpoints_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Infini attention training status report\n",
    "print(\"üìã INFINI ATTENTION TRAINING SESSION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üè† Model: LLaMA 200M with Infini-Attention\")\n",
    "print(f\"üìö Dataset: Preprocessed parquet data\")\n",
    "print(f\"üìÅ Data source: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train/train_data.parquet\")\n",
    "print(f\"‚öôÔ∏è  Configuration: {config_path}\")\n",
    "print()\n",
    "print(\"üî¨ INFINI ATTENTION FEATURES:\")\n",
    "print(f\"   üß© Segment Length: {config.infini_attention.segment_length} tokens\")\n",
    "print(f\"   üß† Compressive Memory: {'Enabled' if config.infini_attention.turn_on_memory else 'Disabled'}\")\n",
    "print(f\"   ‚öñÔ∏è  Balance Factor LR: {config.infini_attention.balance_factor_lr}\")\n",
    "print(f\"   üîÑ Balance Activation: {config.infini_attention.balance_act_type}\")\n",
    "print(f\"   üé® Balance Initialization: {config.infini_attention.balance_init_type}\")\n",
    "print()\n",
    "print(\"üìä TRAINING CONFIGURATION:\")\n",
    "print(f\"   üìè Sequence Length: {config.tokens.sequence_length}\")\n",
    "print(f\"   üî¢ Micro Batch Size: {config.tokens.micro_batch_size}\")\n",
    "print(f\"   üîÑ Training Steps: {config.tokens.train_steps}\")\n",
    "print(f\"   üìà Learning Rate: {config.optimizer.learning_rate_scheduler.learning_rate}\")\n",
    "print(f\"   üíæ Checkpoints: {config.checkpoints.checkpoints_path}\")\n",
    "print()\n",
    "print(\"üìÅ FILES AND PATHS:\")\n",
    "print(f\"   üìÑ Config file: {config_path}\")\n",
    "print(f\"   üìÅ Project directory: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini\")\n",
    "print(f\"   üìö Training data: /Users/zhang/Desktop/huawei/untitled folder 5/nanotron-infini/data/train/\")\n",
    "print(f\"   üíæ Checkpoints: {config.checkpoints.checkpoints_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Infini attention training setup complete!\")\n",
    "\n",
    "print(\"\\nüí° TO USE YOUR OWN DATA:\")\n",
    "print(\"1. üöÄ Run the data preprocessing notebook (data.ipynb) first\")\n",
    "print(\"2. üìÑ Make sure train_data.parquet exists in the data/train/ directory\")\n",
    "print(\"3. üî§ Update the text column name if different from 'text'\")\n",
    "print(\"4. ‚öôÔ∏è  Adjust segment_length based on your data characteristics\")\n",
    "print(\"5. üìä Monitor memory usage and adjust batch_size if needed\")\n",
    "\n",
    "print(\"\\nüî¨ INFINI ATTENTION BENEFITS:\")\n",
    "print(\"‚Ä¢ üß† Infinite context length through compressive memory\")\n",
    "print(\"‚Ä¢ üöÄ Efficient processing with fixed segment sizes\")\n",
    "print(\"‚Ä¢ ‚öñÔ∏è  Learnable balance between local and global attention\")\n",
    "print(\"‚Ä¢ üíæ Memory-efficient for long sequences\")\n",
    "\n",
    "print(\"\\nüìà NEXT STEPS FOR PRODUCTION:\")\n",
    "print(\"1. üîç Scale up training with more data and longer sequences\")\n",
    "print(\"2. üìè Experiment with different segment lengths\")\n",
    "print(\"3. ‚öñÔ∏è  Tune balance factor learning rates\")\n",
    "print(\"4. üìà Monitor attention patterns and memory usage\")\n",
    "print(\"5. üß™ Test on long-context evaluation tasks\")\n",
    "print(\"6. üíæ Save models in formats suitable for inference\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéÜ Happy training with Infini-Attention! üéÜ\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
